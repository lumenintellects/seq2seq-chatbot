{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lumenintellects/seq2seq-chatbot/blob/main/CSK507_EOM_CTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yguDDaQMJZc"
      },
      "source": [
        "#CSK507 End of Module Generative Chatbot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfYarifzMTaV"
      },
      "source": [
        "##1. Import and initialise\n",
        "\n",
        "\n",
        "###1.1 Import\n",
        "First, import everything needed during pre-processing and training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdDj1ol60WD6",
        "outputId": "d228c10d-2750-475c-c6b7-c93a8a8f3d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:38:19.859721\n",
            "Prefferred encoding:  UTF-8\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2024-12-09 12:38:44.129259\n",
            "Time taken to import all packages:  0:00:24.269538\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "overallStart = datetime.datetime.now()\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "import locale\n",
        "print('Prefferred encoding: ',locale.getpreferredencoding())\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "import os #needed to manipulate the downloaded files within the collab environment and to pull data\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import zipfile\n",
        "import spacy.cli\n",
        "from spacy.lang.en import English # updated\n",
        "gpu = spacy.prefer_gpu()\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from ipywidgets import IntProgress\n",
        "from IPython.display import display\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to import all packages: ',end-start)\n",
        "\n",
        "\n",
        "\n",
        "runtype = 'ubuntu' #'ubuntu' or 'wiki'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNs6U_CWNO-3"
      },
      "source": [
        "##1.2 Download and unzip dataset\n",
        "We need to pull the dataset down and unzip from https://www.microsoft.com/en-us/download/confirmation.aspx?id=52419"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UljUgoMO0lza",
        "outputId": "11156186-de74-46c6-8727-0612e22e15ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:38:44.141563\n",
            "zip doesnt exist, downloading\n",
            "--2024-12-09 12:38:44--  https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 23.200.181.207, 2600:1417:3f:59b::317f, 2600:1417:3f:580::317f\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|23.200.181.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7094233 (6.8M) [application/octet-stream]\n",
            "Saving to: ‘content/WikiQACorpus.zip’\n",
            "\n",
            "WikiQACorpus.zip    100%[===================>]   6.76M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-12-09 12:38:44 (56.1 MB/s) - ‘content/WikiQACorpus.zip’ saved [7094233/7094233]\n",
            "\n",
            "zip not unzipped, unzipping\n",
            "2024-12-09 12:38:44.590409\n",
            "Time taken to download and unzip:  0:00:00.448846\n"
          ]
        }
      ],
      "source": [
        "#https://www.microsoft.com/en-us/download/confirmation.aspx?id=52419\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'wiki':\n",
        "  if os.path.exists('content/WikiQACorpus.zip') == False:\n",
        "    print('zip doesn''t exist, downloading')\n",
        "    !wget -P content https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\n",
        "  else:\n",
        "    print('WikiQACorpus.tsv already exists, not need to download')\n",
        "\n",
        "  if os.path.exists('content/WikiQACorpus/WikiQA.tsv') == False:\n",
        "    print('zip not unzipped, unzipping')\n",
        "    with zipfile.ZipFile('content/WikiQACorpus.zip', 'r') as zip_ref:\n",
        "      zip_ref.extractall('content')\n",
        "  else:\n",
        "    print('WikiQA.tsv already exists, not need to unzip')\n",
        "else\n",
        "  print('Not running wikiQA corpus')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to download and unzip: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE6WlKLFOB73"
      },
      "source": [
        "##1.3 Pandas DF\n",
        "Put data into pandas df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQDcAX452DeW",
        "outputId": "0a44d43a-64b8-4ac8-f89a-fd6d6bf38d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:38:44.597653\n",
            "2024-12-09 12:38:44.785008\n",
            "Time taken to load to df:  0:00:00.187355\n"
          ]
        }
      ],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'wiki':\n",
        "  df = pd.read_csv('content/WikiQACorpus/WikiQA.tsv',on_bad_lines='skip',engine='python',sep='\\t')\n",
        "  end = datetime.datetime.now()\n",
        "  #df = df.loc[0:1000]\n",
        "else\n",
        "  print('Not running wikiQA corpus')\n",
        "\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to load to df: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMKXnMxkOKWM"
      },
      "source": [
        "#2. Text normalisation, cleaning and extraction\n",
        "\n",
        "##2.1 Normalise using NFKD\n",
        "We first have to nomalise the data using NFKD to make standard alpha so that _é_ becomes _e_ etc. Additionally, check the coding by converting to ascii and back to UTF-8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R1f_dU24pqM",
        "outputId": "93fcbab8-842c-4d89-84e9-0284fe26323a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:38:44.792687\n",
            "2024-12-09 12:38:44.871904\n",
            "Time taken to nomalise:  0:00:00.079217\n"
          ]
        }
      ],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'wiki':\n",
        "  df['Question'] = df.Question.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') # this will make the text normal alpha, for exaple é will be e, etc\n",
        "  df['Sentence'] = df.Sentence.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') # this will make the text normal alpha, for exaple é will be e, etc\n",
        "else\n",
        "  print('Not running wikiQA corpus')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to nomalise: ',end-start)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEsTE1HHTZMR"
      },
      "source": [
        "##2.2 Clean data\n",
        "Next we clean the data by removing triple and double spaces befre making lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOvIBlzEQOMF",
        "outputId": "4837c5e9-e3bf-4c8c-e84e-aa544fefb06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:38:44.879578\n",
            "2024-12-09 12:38:44.949762\n",
            "Time taken to clean:  0:00:00.070184\n"
          ]
        }
      ],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'wiki':\n",
        "  df['Question'] = df['Question'].str.replace('\\n',' ') # remove carriage returns\n",
        "  df['Question'] = df['Question'].str.replace('   ',' ') #remove triple spaces\n",
        "  df['Question'] = df['Question'].str.replace('  ',' ') # also remove double sapces (which might remain after triples are removed)\n",
        "  df['Question'] = df['Question'].str.lower()\n",
        "  df['Sentence'] = df['Sentence'].str.replace('\\n',' ') # remove carriage returns\n",
        "  df['Sentence'] = df['Sentence'].str.replace('   ',' ') #remove triple spaces\n",
        "  df['Sentence'] = df['Sentence'].str.replace('  ',' ') # also rmeove double sapces (which might remain after triples are removed)\n",
        "  df['Sentence'] = df['Sentence'].str.lower()\n",
        "else\n",
        "  print('Not running wikiQA corpus')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to clean: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10V8qcFwU1Ku"
      },
      "source": [
        "##2.3 Data Extraction\n",
        "Next we extract the relevant normalised and cleansed columns of dataframe into ques and ans dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLJBIEVYPrGk",
        "outputId": "8ba6d16f-161b-4353-878a-23cce7c5de89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:38:44.956053\n",
            "2024-12-09 12:38:44.956418\n",
            "Time taken to extract:  0:00:00.000365\n"
          ]
        }
      ],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'wiki':\n",
        "  ques = df['Question']\n",
        "  ans = df['Sentence']\n",
        "else\n",
        "  print('Not running wikiQA corpus')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to extract: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf2X0lPpkBIA"
      },
      "source": [
        "#2.4 Export to csv for team wide review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoBg9_eSh7Jk"
      },
      "outputs": [],
      "source": [
        "if runtype == 'wiki':\n",
        "  outputdf = pd.DataFrame({'input': ques, 'output': ans})\n",
        "  outputdf.to_csv('outputdf.csv')\n",
        "else\n",
        "  print('Not running wikiQA corpus')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9gdA7VurWCe"
      },
      "source": [
        "##2.5 Switch to Ubuntu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUKgXikirekt",
        "outputId": "4d38a1e7-5213-40db-f924-a1c34bb046f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:38:45.101486\n",
            "length of df before splitting: 7636876\n",
            "length of df after splitting: 38185\n",
            "2024-12-09 12:39:18.801219\n",
            "Time taken to load to df:  0:00:33.699733\n"
          ]
        }
      ],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'ubuntu':\n",
        "  dfu = pd.read_csv('/content/drive/MyDrive/ubuntu_dialogue_corpus_input_output_pairs.csv',on_bad_lines='skip',engine='python')\n",
        "\n",
        "\n",
        "\n",
        "  print('length of df before splitting:', len(dfu))\n",
        "\n",
        "  first_PCT = int(len(dfu)*0.005)\n",
        "  df = dfu.loc[0:first_PCT]\n",
        "\n",
        "  print('length of df after splitting:', len(df))\n",
        "\n",
        "\n",
        "  ques = df['input']\n",
        "  ans = df['output']\n",
        "  df = pd.DataFrame({'input': ques, 'output': ans})\n",
        "else\n",
        "  print('Not running ubuntu corpus')\n",
        "\n",
        "#df = df.loc[0:100]\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to load to df: ',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L-6Ka8UgXtPC",
        "outputId": "a9197700-bc95-46ca-eb84-e68abc02aee0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               input  \\\n",
              "0                                  sock puppe?t wtf?   \n",
              "1                         probably not a vector icon   \n",
              "2                    how do fix vlc in the terminal?   \n",
              "3  ok, anyone know how i set bios password in ubu...   \n",
              "4  how do i generate an xorg.conf file? for intel...   \n",
              "\n",
              "                                              output  \n",
              "0  it's a wikipedia term. there is prodigy, prdig...  \n",
              "1        the other icons aren't vector icons either.  \n",
              "2  fix what? check other programs. same faint sound?  \n",
              "3               you'd need to set that in your bios.  \n",
              "4                    'sudo xorg -configure', i think  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29a8fd8f-d98d-4f89-927d-470f667ee459\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sock puppe?t wtf?</td>\n",
              "      <td>it's a wikipedia term. there is prodigy, prdig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>probably not a vector icon</td>\n",
              "      <td>the other icons aren't vector icons either.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how do fix vlc in the terminal?</td>\n",
              "      <td>fix what? check other programs. same faint sound?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ok, anyone know how i set bios password in ubu...</td>\n",
              "      <td>you'd need to set that in your bios.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>how do i generate an xorg.conf file? for intel...</td>\n",
              "      <td>'sudo xorg -configure', i think</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29a8fd8f-d98d-4f89-927d-470f667ee459')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29a8fd8f-d98d-4f89-927d-470f667ee459 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29a8fd8f-d98d-4f89-927d-470f667ee459');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a57711be-5b93-4dfc-ace8-4f63a6cfe562\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a57711be-5b93-4dfc-ace8-4f63a6cfe562')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a57711be-5b93-4dfc-ace8-4f63a6cfe562 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 38185,\n  \"fields\": [\n    {\n      \"column\": \"input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 37176,\n        \"samples\": [\n          \"imho,there's difference between crap to crap :)\",\n          \"gutsy discussions in #ubuntu+1 channel\",\n          \"cp -rp /home/olduser/* /home/newuser/*; cp -rp /home/olduser/.* /home/newuser/*\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 33937,\n        \"samples\": [\n          \"you mean thee scrollback in irc?\",\n          \"01:08.0 ethernet controller: intel corporation 82801ba/bam/ca/cam ethernet controller (rev 01) ?\",\n          \"no, just refresh the package lists\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDSH1m9UVLQH"
      },
      "source": [
        "#3. Tokenization and indexing\n",
        "\n",
        "Next we need to tokenize the data using spaCy before making word-to-index and index-to-word mappings. Finally we will need to convert toindices for the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "544SRqLvTr6Y"
      },
      "source": [
        "##3.1 Import spaCy Pipeline\n",
        "\n",
        "Large Class used because of copus complexity, en_core_web_lg contains 343k unique vectors, compared with en_core_web_sm which only has 20k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEx2LiG5L5vA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41816aef-681c-4fec-94b0-5d3a8019e9b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:39:18.903391\n",
            "Downloading en_core_web_lg\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:39:37.926194\n",
            "Time taken to download and load spaCy class:  0:00:19.022803\n"
          ]
        }
      ],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "#because of the complex corpus, use the larger en_core_web_lg\n",
        "\n",
        "try:\n",
        "  nlp = spacy.load(\"en_core_web_lg\")\n",
        "except OSError:\n",
        "  print('Downloading en_core_web_lg')\n",
        "  spacy.cli.download(\"en_core_web_lg\")\n",
        "  nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to download and load spaCy class: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urh0je79WRTY"
      },
      "source": [
        "##3.2 Counters\n",
        "\n",
        "We need a way of getting and storing the frequency, so we need to initalise counters (from collections package)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpaE6JYlWQMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352a773e-f8a6-4347-e74d-63d042b2feaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:39:37.934617\n",
            "2024-12-09 12:39:37.934863\n",
            "Time taken to initialise counters:  0:00:00.000246\n"
          ]
        }
      ],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ques_words = Counter()\n",
        "ans_words = Counter()\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to initialise counters: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRQPxKaDWyLE"
      },
      "source": [
        "##3.3 Tokenize\n",
        "\n",
        "totokenize, we use spaCy pieline already loaded and go through the entire df row by row before stitching _EOS onto the end for the seq-to-seq model later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLJPx5n6XTey"
      },
      "outputs": [],
      "source": [
        "#start = datetime.datetime.now()\n",
        "#print(start)\n",
        "#longString = ' '.join(df.input.astype(str).tolist()) #get all comments into string\n",
        "#print('cp1')\n",
        "#lowerString = ' '.join(longString.lower().split())\n",
        "#print('cp2 ')\n",
        "#words = lowerString.split()\n",
        "#print('cp3')\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyKKtpdab1sg"
      },
      "outputs": [],
      "source": [
        "##print(longString)\n",
        "#len(longString)\n",
        "#dfw = pd.DataFrame({'word': list(words)})\n",
        "#dfw.drop_duplicates(inplace=True)\n",
        "#dfw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuUHSslBwN7h"
      },
      "outputs": [],
      "source": [
        "#ques = ques.apply(lambda x: str(x) + ' _EOS')\n",
        "#ques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7PHI-21bVMy"
      },
      "outputs": [],
      "source": [
        "#print(dfw)\n",
        "#terms = np.unique(words)\n",
        "#print('cp4')\n",
        "#\n",
        "#terms = np.sort(terms, axis=None)\n",
        "#end = datetime.datetime.now()\n",
        "#print(end)\n",
        "#\n",
        "#print('Time taken to get BoW: ',end-start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv02xT7m5IK7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8a3d07-70f7-4c78-b0d4-89e47ad70b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:49:12.891145\n",
            "2024-12-09 12:49:12.891400\n",
            "CSV files already exist. Skipping tokenization.\n",
            "2024-12-09 12:49:15.011198\n",
            "Time taken to tokenize:  0:00:02.119798\n"
          ]
        }
      ],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from ipywidgets import IntProgress\n",
        "from IPython.display import display\n",
        "import spacy\n",
        "\n",
        "# ... (rest of your imports and code)\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "# Check if the CSV files already exist\n",
        "csv_files_exist = all([\n",
        "    os.path.exists('/content/drive/MyDrive/ques_inputs.csv'),\n",
        "    os.path.exists('/content/drive/MyDrive/ans_inputs.csv'),\n",
        "    os.path.exists('/content/drive/MyDrive/ques_words.csv'),\n",
        "    os.path.exists('/content/drive/MyDrive/ans_words.csv')\n",
        "])\n",
        "\n",
        "if csv_files_exist:\n",
        "    print(\"CSV files already exist. Skipping tokenization.\")\n",
        "    # Load the data from the CSV files\n",
        "    ques_inputs = pd.read_csv('/content/drive/MyDrive/ques_inputs.csv').values.tolist()\n",
        "    ans_inputs = pd.read_csv('/content/drive/MyDrive/ans_inputs.csv').values.tolist()\n",
        "    ques_words = pd.read_csv('/content/drive/MyDrive/ques_words.csv').values.tolist()\n",
        "    ans_words = pd.read_csv('/content/drive/MyDrive/ans_words.csv').values.tolist()\n",
        "else:\n",
        "    print(\"CSV files not found. Starting tokenization.\")\n",
        "    gpu = spacy.prefer_gpu() #GPU was found to be slightly better peforming in colab for SpaCy. The GPU selection needs to be immediately before the load command so need to reload\n",
        "    nlp = spacy.load(\"en_core_web_lg\") # large was used\n",
        "\n",
        "    ques_inputs = []\n",
        "    ans_inputs = []\n",
        "\n",
        "    f = IntProgress(min=0, max=len(df)) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "    display(f) # display the bar\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        ques_tokens = nlp(str(ques[i]))\n",
        "        ans_tokens = nlp(str(ans[i]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if (len(ques_tokens)!=0 and len(ans_tokens)!=0):\n",
        "\n",
        "\n",
        "          for token in ques_tokens:\n",
        "              ques_words.update([token.text]) #this is the counter for the question frequency, update it\n",
        "\n",
        "\n",
        "          ques_inputs.append([token.text for token in ques_tokens] + ['_EOS'])\n",
        "\n",
        "          for token in ans_tokens:\n",
        "                ans_words.update([token.text]) #this is the counter for the answer frequency, update it\n",
        "\n",
        "          ans_inputs.append([token.text for token in ans_tokens] + ['_EOS'])\n",
        "        f.value += 1\n",
        "    pd.DataFrame(ques_inputs).to_csv('/content/drive/MyDrive/ques_inputs.csv', index=False)\n",
        "    pd.DataFrame(ans_inputs).to_csv('/content/drive/MyDrive/ans_inputs.csv', index=False)\n",
        "    pd.DataFrame(ques_words).to_csv('/content/drive/MyDrive/ques_words.csv', index=False)\n",
        "    pd.DataFrame(ans_words).to_csv('/content/drive/MyDrive/ans_words.csv', index=False)\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to tokenize: ',end-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRTPyF3WuRtN",
        "outputId": "ca1909de-6533-4936-bb7a-1682c768fad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[352, 40, 7, 4961, 564, 1]\n",
            "[5, 185, 1322, 37, 20, 3167, 1322, 333, 7, 1]\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(ques_inputs)):\n",
        "    ques_inputs[i] = [int(x) for x in ques_inputs[i] if x != '' and not pd.isnull(x)]\n",
        "\n",
        "for i in range(len(ans_inputs)):\n",
        "    ans_inputs[i] = [int(x) for x in ans_inputs[i] if x != '' and not pd.isnull(x)]\n",
        "\n",
        "print(ques_inputs[1])\n",
        "print(ans_inputs[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTRXje7a0zU"
      },
      "source": [
        "##3.4 Vocabulary creation\n",
        "\n",
        "Next we need to create word-to-index and index-to-word for both questions and answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONeEQ3nirM0q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "bff7a571aa1d4a77948d759990ab5e04",
            "5e679cf5f6f944e48464ca5a61b62a24",
            "a6ef5e403e174f7b86bfeda545204d3e"
          ]
        },
        "id": "hLbyd_Q8avs1",
        "outputId": "45bade5a-1392-4048-9ab4-9fb1d415a242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:39:44.131860\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntProgress(value=0, max=38185)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bff7a571aa1d4a77948d759990ab5e04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 12:39:55.508576\n",
            "Time taken to create vocabulary:  0:00:11.376716\n"
          ]
        }
      ],
      "source": [
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "\n",
        "#ques_words = ['_SOS','_EOS','_UNK'] + sorted(ques_words,key=ques_words.get,reverse=True) #add 'special words' like EOS to the entire word list\n",
        "#ans_words = ['_SOS','_EOS','_UNK'] + sorted(ans_words,key=ans_words.get,reverse=True)\n",
        "\n",
        "ques_words = [item for sublist in ques_words for item in sublist]\n",
        "ans_words = [item for sublist in ans_words for item in sublist]\n",
        "\n",
        "ques_w2i = {a:i for i,a in enumerate(ques_words)} #record the index against each word with the word first in a dictionary\n",
        "ans_w2i = {a:i for i,a in enumerate(ans_words)}\n",
        "\n",
        "\n",
        "ques_i2w = {i:o for i,o in enumerate(ques_words)} #do the same thing but in reverse so the index is first\n",
        "ans_i2w = {i:o for i,o in enumerate(ans_words)}\n",
        "\n",
        "g = IntProgress(min=0, max=len(ques_inputs)) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "display(g) # display the bar\n",
        "\n",
        "#for i in range(len(ques_inputs)):\n",
        "for i in range(len(ans_inputs)):\n",
        "    ques_sentence = str(ques_inputs[i])\n",
        "    ans_sentence = str(ans_inputs[i])\n",
        "    ques_inputs[i] = [ques_w2i.get(word, ques_w2i['_UNK']) for word in ques_inputs[i]]  # Use get to handle missing words, replace with _UNK token\n",
        "    ans_inputs[i] = [ans_w2i.get(word, ans_w2i['_UNK']) for word in ans_inputs[i]]  # Use get to handle missing words, replace with _UNK token\n",
        "    g.value += 1\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to create vocabulary: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuSh-YJ2gzXB"
      },
      "source": [
        "#lifted straight from CSK507 coursework:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imfEpfxeDt47"
      },
      "outputs": [],
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, vocab_len, input_dim, hidden_dim, n_layers=1, drop_prob=0):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_len, input_dim)\n",
        "    self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers,\n",
        "                        dropout=drop_prob, batch_first=True)\n",
        "\n",
        "  def forward(self, inputs, encoder_state_vector, encoder_cell_vector):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embedded = self.embedding(inputs).to(device)\n",
        "    # Pass the embedded word vectors into LSTM and return all outputs\n",
        "    output, hidden = self.lstm(embedded, (encoder_state_vector, encoder_cell_vector))\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size=1):\n",
        "    return (torch.zeros(self.n_layers, batch_size,\n",
        "                        self.hidden_dim),\n",
        "            torch.zeros(self.n_layers, batch_size,\n",
        "                        self.hidden_dim))\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_vocab_len, n_layers=1, drop_prob=0.1):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_vocab_len = output_vocab_len\n",
        "    self.n_layers = n_layers\n",
        "    self.drop_prob = drop_prob\n",
        "    self.input_dim = input_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(self.output_vocab_len, self.input_dim)\n",
        "    self.dropout = nn.Dropout(self.drop_prob)\n",
        "    self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, batch_first=True)\n",
        "    self.classifier = nn.Linear(self.hidden_dim, self.output_vocab_len)\n",
        "\n",
        "  def forward(self, inputs, decoder_state_vector, decoder_context_vector):\n",
        "    # Embed input words\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embedded = self.embedding(inputs).view(1, -1).to(device)\n",
        "    embedded = self.dropout(embedded)\n",
        "    embedded = embedded.unsqueeze(0)\n",
        "\n",
        "    output, hidden = self.lstm(embedded, (decoder_state_vector,\n",
        "                                          decoder_context_vector))\n",
        "\n",
        "    # Pass LSTM outputs through a Linear layer acting as a classifier\n",
        "    output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
        "\n",
        "    return output, hidden\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_w-5Xuxg8uC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "ff76722aaf044266897e5570d33587f7",
            "618fe8c495264c84994c737f542a6cde",
            "c5af0419a6eb4c5da0a793d5449d5acf",
            "b2f3d57bd0194c998ceef72809719a2f",
            "537a17032f4f4f23b03c9e08c1178c07",
            "a330ca19f9524fff9e4516f389bba209"
          ]
        },
        "outputId": "75bb0d8a-36ab-416e-f522-2ea100bc11eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 13:20:07.853206\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-c75561ac8a1f>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"/content/drive/MyDrive/model_enc_dec.pt\", map_location=torch.device(device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing model loaded.\n",
            "overall training progress\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntProgress(value=0, max=4012060)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff76722aaf044266897e5570d33587f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch  1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntProgress(value=0, max=401206)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2f3d57bd0194c998ceef72809719a2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "start = datetime.datetime.now()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(start)\n",
        "print()\n",
        "#i=0\n",
        "input_dim = 10000\n",
        "hidden_dim = 256\n",
        "spacy.prefer_gpu()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = EncoderLSTM(len(ques_words), input_dim, hidden_dim).to(device)\n",
        "decoder = DecoderLSTM(input_dim, hidden_dim, len(ans_words)).to(device)\n",
        "\n",
        "lr = 0.001\n",
        "#encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "#decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "if os.path.exists(\"/content/drive/MyDrive/model_enc_dec.pt\"):\n",
        "    # Load the existing model\n",
        "    checkpoint = torch.load(\"/content/drive/MyDrive/model_enc_dec.pt\", map_location=torch.device(device))\n",
        "    encoder.load_state_dict(checkpoint['encoder'])\n",
        "    decoder.load_state_dict(checkpoint['decoder'])\n",
        "    ##encoder_optimizer.load_state_dict(checkpoint['q_optimizer'])\n",
        "    #decoder_optimizer.load_state_dict(checkpoint['a_optimizer'])\n",
        "    print(\"Existing model loaded.\")\n",
        "else:\n",
        "    # Create a new model\n",
        "    encoder = EncoderLSTM(len(ques_words), input_dim, hidden_dim).to(device)\n",
        "    decoder = DecoderLSTM(input_dim, hidden_dim, len(ans_words)).to(device)\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "    print(\"New model created.\")\n",
        "\n",
        "EPOCHS = 10\n",
        "teacher_forcing_prob = 0.5\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "tk0 = range(1,EPOCHS+1)\n",
        "tk2 = enumerate(ques_inputs)\n",
        "max_index = 0\n",
        "for b, sentence in tk2:\n",
        "  for bb in range(len(ans_inputs[b])):\n",
        "    max_index += 1\n",
        "\n",
        "k = IntProgress(min=0, max=max_index * EPOCHS) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "print('overall training progress')\n",
        "print()\n",
        "display(k)\n",
        "print()\n",
        "for epoch in tk0:\n",
        "    avg_loss = 0.\n",
        "    j = IntProgress(min=0, max=max_index) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "    print('epoch ', epoch)\n",
        "    display(j) # display the bar\n",
        "    print()\n",
        "    tk1 = enumerate(ques_inputs)\n",
        "    for i, sentence in tk1:\n",
        "        #print(i)\n",
        "        #print(sentence)\n",
        "        loss = 0.\n",
        "\n",
        "        #initialise encoder state vector and cell state vector\n",
        "        h = encoder.init_hidden()\n",
        "        encoder_state_vector = h[0]\n",
        "        encoder_state_vector = encoder_state_vector.to(device)\n",
        "        encoder_cell_vector = h[0]\n",
        "        encoder_cell_vector = encoder_cell_vector.to(device)\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        inp = torch.tensor(sentence).unsqueeze(0).to(device)\n",
        "\n",
        "        #print('inp: ', epoch, inp)\n",
        "        #if (i % 1000) == 0:\n",
        "        #  print('inp: ', i, epoch)\n",
        "        encoder_outputs, h = encoder(inp, encoder_state_vector, encoder_cell_vector)\n",
        "\n",
        "        #First decoder input will be the SOS token\n",
        "        decoder_input = torch.tensor([ques_w2i['_SOS']]).to(device)\n",
        "        #First decoder hidden state will be last encoder hidden state\n",
        "        decoder_hidden = h\n",
        "\n",
        "        output = []\n",
        "        teacher_forcing = True if random.random() < teacher_forcing_prob else False\n",
        "        #k = IntProgress(min=0, max=len(ans_inputs[i])) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "        #k.value=0\n",
        "        #display(k) # display the bar\n",
        "\n",
        "        for ii in range(len(ans_inputs[i])):\n",
        "          decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden[0], decoder_hidden[1])\n",
        "\n",
        "          # Get the index value of the word with the highest score from the decoder output\n",
        "          top_value, top_index = decoder_output.topk(1)\n",
        "          if teacher_forcing:\n",
        "            decoder_input = torch.tensor([ans_inputs[i][ii]]).to(device)\n",
        "          else:\n",
        "            decoder_input = torch.tensor([top_index.item()]).to(device)\n",
        "\n",
        "          output.append(top_index.item())\n",
        "          # Calculate the loss of the prediction against the actual word\n",
        "          loss += F.nll_loss(decoder_output.view(1,-1), torch.tensor([ans_inputs[i][ii]]).to(device))\n",
        "          j.value += 1\n",
        "          k.value += 1\n",
        "          #print(j.value)\n",
        "          #print((j.value/(len(ans_inputs) * max_index))*100)\n",
        "\n",
        "        loss.backward()\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "        avg_loss += loss.item()/len(ques_inputs)\n",
        "    print(avg_loss)\n",
        "    torch.save({\"encoder\":encoder.state_dict(),\"decoder\":decoder.state_dict(),\"q_optimizer\":encoder_optimizer.state_dict(),\"a_optimizer\":decoder_optimizer},\"/content/drive/MyDrive/model_enc_dec.pt\")\n",
        "\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to train model: ',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "h7asNohDkPkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0a2d54-84f3-44f8-8dea-497c792ba8f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-09 13:18:58.992081\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-bc17726db70c>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"/content/drive/MyDrive/model_enc_dec.pt\", map_location=torch.device(device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing model loaded.\n",
            "inp: tensor([[179, 265, 391,   5,  95, 142,  26,  39,   6, 373, 128,  15, 265,   1]],\n",
            "       device='cuda:0')\n",
            "en_inputs[ 25538 ]:  [179, 265, 391, 5, 95, 142, 26, 39, 6, 373, 128, 15, 265, 1]\n",
            "Question: well dapper seems to run ok anyone know the xorg version in dapper _EOS\n",
            "Answer Predicted: i is a lot\n",
            "Actual: 7.0 _EOS\n",
            "\n",
            "inp: tensor([[ 59,  16,   4,  30,   7, 106,  16,  12,  14,   4, 146,  49, 779, 551,\n",
            "           5, 356, 551, 335,   6, 375,   3,   1]], device='cuda:0')\n",
            "en_inputs[ 13523 ]:  [59, 16, 4, 30, 7, 106, 16, 12, 14, 4, 146, 49, 779, 551, 5, 356, 551, 335, 6, 375, 3, 1]\n",
            "Question: hello . i have a question . how do i upgrade from 6.06 lts to 8.04 lts via the shell ? _EOS\n",
            "Answer Predicted: i\n",
            "Actual: you can try , but i would not ... it will probably get messed up i have played it before . _EOS\n",
            "\n",
            "inp: tensor([[ 88, 365,   1]], device='cuda:0')\n",
            "en_inputs[ 3943 ]:  [88, 365, 1]\n",
            "Question: # hardware _EOS\n",
            "Answer Predicted: thanks\n",
            "Actual: & ompaul : thanks _EOS\n",
            "\n",
            "inp: tensor([[816,   1]], device='cuda:0')\n",
            "en_inputs[ 30067 ]:  [816, 1]\n",
            "Question: ifconfig _EOS\n",
            "Answer Predicted: thanks\n",
            "Actual: cool , _EOS\n",
            "\n",
            "inp: tensor([[ 59,  58,   8,   4,  30,   7, 106,   8,  13,   4, 331,  23,   7, 325,\n",
            "         166,   3,   1]], device='cuda:0')\n",
            "en_inputs[ 2304 ]:  [59, 58, 8, 4, 30, 7, 106, 8, 13, 4, 331, 23, 7, 325, 166, 3, 1]\n",
            "Question: hello all , i have a question , can i write on a ntfs partition ? _EOS\n",
            "Answer Predicted: i\n",
            "Actual: without going mad , no _EOS\n",
            "\n",
            "inp: tensor([[   4,  389,  165, 1770,    8,   10,  206,  304,  179,   29,  374,  231,\n",
            "          716,    1]], device='cuda:0')\n",
            "en_inputs[ 19623 ]:  [4, 389, 165, 1770, 8, 10, 206, 304, 179, 29, 374, 231, 716, 1]\n",
            "Question: i recommend trying wicd , it works very well with most wireless cards _EOS\n",
            "Answer Predicted: i ,\n",
            "Actual: what is wicd ? how do i try it ? _EOS\n",
            "\n",
            "inp: tensor([[9236,  383,  508,   42,  237,   42,  772, 3455, 9237,   23,   58,   77,\n",
            "           16,  449,  267,   15,  237,  206,  528,   16,   45,  381,    3,  443,\n",
            "          418,    1]], device='cuda:0')\n",
            "en_inputs[ 60 ]:  [9236, 383, 508, 42, 237, 42, 772, 3455, 9237, 23, 58, 77, 16, 449, 267, 15, 237, 206, 528, 16, 45, 381, 3, 443, 418, 1]\n",
            "Question: ubutntu 9.04 beta : compiz : missing transparency effeccts on all windows . everything else in compiz works great . any ideas ? intel graphics _EOS\n",
            "Answer Predicted: i ,\n",
            "Actual: # ubuntu+1 _EOS\n",
            "\n",
            "inp: tensor([[   26,    39,    64,     5,    91,     6, 14129,    19,  2671,    32,\n",
            "          5338,     3,     1]], device='cuda:0')\n",
            "en_inputs[ 12664 ]:  [26, 39, 64, 5, 91, 6, 14129, 19, 2671, 32, 5338, 3, 1]\n",
            "Question: anyone know where to find the server.met for amule / emule ? _EOS\n",
            "Answer Predicted: i\n",
            "Actual: hold on i used to get that http://ed2k.2x4u.de/index.html _EOS\n",
            "\n",
            "inp: tensor([[  12,   14,    4,   34,   54, 1219,  128,   28,    7,  130,    8,   19,\n",
            "          723,    8,   54,  377,  128,   24, 1547,   29,    6,  865,  271,    3,\n",
            "            1]], device='cuda:0')\n",
            "en_inputs[ 4021 ]:  [12, 14, 4, 34, 54, 1219, 128, 28, 7, 130, 8, 19, 723, 8, 54, 377, 128, 24, 1547, 29, 6, 865, 271, 3, 1]\n",
            "Question: how do i install an older version of a program , for example , an old version that came with the warty release ? _EOS\n",
            "Answer Predicted: i ,\n",
            "Actual: search for the .deb a la google _EOS\n",
            "\n",
            "inp: tensor([[  31,    9,   63, 5570,  469,   22,   10,  209,  270,   38,  554,    1]],\n",
            "       device='cuda:0')\n",
            "en_inputs[ 8089 ]:  [31, 9, 63, 5570, 469, 22, 10, 209, 270, 38, 554, 1]\n",
            "Question: there is no nginx process and it still wo n't uninstall _EOS\n",
            "Answer Predicted: thanks\n",
            "Actual: i 'm not familiar with it , but if it does nt load on boot , it probaly does no harm . _EOS\n",
            "\n",
            "inp: tensor([[4910,  497,  787,   42, 3449, 3726,  247,  890,   57, 8636, 1021,  138,\n",
            "         8637,   55,  891,   57, 4910,  433,  787, 3449, 3726,  247,  890,   57,\n",
            "         8636, 1021,  138, 8637,   55,  891,   55,    1]], device='cuda:0')\n",
            "en_inputs[ 23717 ]:  [4910, 497, 787, 42, 3449, 3726, 247, 890, 57, 8636, 1021, 138, 8637, 55, 891, 57, 4910, 433, 787, 3449, 3726, 247, 890, 57, 8636, 1021, 138, 8637, 55, 891, 55, 1]\n",
            "Question: bob2 last seen : 23 weeks 2 days ( 7h 8 m 13s ) ago ( bob2 ` seen 23 weeks 2 days ( 7h 8 m 13s ) ago ) _EOS\n",
            "Answer Predicted: i\n",
            "Actual: how can i ensure that my rc candidate will get the proper upgrade ? apt - get update ; apt - get upgrade ? _EOS\n",
            "\n",
            "inp: tensor([[  4,  14,  38, 461,   5,  30,  24,  70,   1]], device='cuda:0')\n",
            "en_inputs[ 27650 ]:  [4, 14, 38, 461, 5, 30, 24, 70, 1]\n",
            "Question: i do n't seem to have that command _EOS\n",
            "Answer Predicted: i\n",
            "Actual: need to install ppa - purge first _EOS\n",
            "\n",
            "inp: tensor([[ 115,    9,    6,  374, 2337, 1476,   19,   11,    8,  654,    5,    6,\n",
            "         1438, 1476,    3,   22,   64,   13,    4,   36,   10,    3,    3,    1]],\n",
            "       device='cuda:0')\n",
            "en_inputs[ 3457 ]:  [115, 9, 6, 374, 2337, 1476, 19, 11, 8, 654, 5, 6, 1438, 1476, 3, 22, 64, 13, 4, 36, 10, 3, 3, 1]\n",
            "Question: which is the most popular dock for ubuntu , similar to the osx dock ? and where can i get it ? ? _EOS\n",
            "Answer Predicted: i\n",
            "Actual: buy a mac _EOS\n",
            "\n",
            "inp: tensor([[ 33, 432, 637, 209, 162,   6,  21, 102,  21, 139, 200, 392,   9, 192,\n",
            "          58,   4,  61,   5,  39,   3,   1]], device='cuda:0')\n",
            "en_inputs[ 11890 ]:  [33, 432, 637, 209, 162, 6, 21, 102, 21, 139, 200, 392, 9, 192, 58, 4, 61, 5, 39, 3, 1]\n",
            "Question: does feisty alternative still support the ' server ' boot line option is really all i need to know ? _EOS\n",
            "Answer Predicted: i\n",
            "Actual: there is a server install disc _EOS\n",
            "\n",
            "inp: tensor([[  11,    9,    7, 2176,  323,    8,   22,  431, 3574,   72,  291,  222,\n",
            "            5,   77,   72,    1]], device='cuda:0')\n",
            "en_inputs[ 29243 ]:  [11, 9, 7, 2176, 323, 8, 22, 431, 3574, 72, 291, 222, 5, 77, 72, 1]\n",
            "Question: ubuntu is a crappy os , and bad suport .. going back to windows .. _EOS\n",
            "Answer Predicted: i ,\n",
            "Actual: stop that please your welcome to not use it and use windows if you do n't like it _EOS\n",
            "\n",
            "inp: tensor([[1344,  185,   22,   34,  294,    1]], device='cuda:0')\n",
            "en_inputs[ 14373 ]:  [1344, 185, 22, 34, 294, 1]\n",
            "Question: purge remove and install again _EOS\n",
            "Answer Predicted: i\n",
            "Actual: when i tried all that unity would n't install claiming it wanted different compiz packages _EOS\n",
            "\n",
            "inp: tensor([[   26,    89,    11,    23,  6928,   741,    29, 20509,   348,     3,\n",
            "             1]], device='cuda:0')\n",
            "en_inputs[ 29109 ]:  [26, 89, 11, 23, 6928, 741, 29, 20509, 348, 3, 1]\n",
            "Question: anyone installed ubuntu on emachines netbook with 1bm ram ? _EOS\n",
            "Answer Predicted: i\n",
            "Actual: emachines makes a lot of machines ... a 1 gb ram box . should work . _EOS\n",
            "\n",
            "inp: tensor([[ 352, 6663,   13,    1]], device='cuda:0')\n",
            "en_inputs[ 18921 ]:  [352, 6663, 13, 1]\n",
            "Question: probably lindvd can _EOS\n",
            "Answer Predicted: i ,\n",
            "Actual: thanks i 'll look into it _EOS\n",
            "\n",
            "inp: tensor([[ 179,   24,   37,   20, 1154,    6,  497,  693,  984,    4,  184, 2274,\n",
            "          373,   16,    1]], device='cuda:0')\n",
            "en_inputs[ 21215 ]:  [179, 24, 37, 20, 1154, 6, 497, 693, 984, 4, 184, 2274, 373, 16, 1]\n",
            "Question: well that 's what happened the last few times i 've edited xorg . _EOS\n",
            "Answer Predicted: i ,\n",
            "Actual: then you did something incorrectly . _EOS\n",
            "\n",
            "2024-12-09 13:19:12.186763\n",
            "Time taken to test model:  0:00:13.194682\n"
          ]
        }
      ],
      "source": [
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/model_enc_dec.pt\", map_location=torch.device(device))\n",
        "\n",
        "if os.path.exists(\"/content/drive/MyDrive/model_enc_dec.pt\"):\n",
        "    # Load the existing model\n",
        "    #checkpoint = torch.load(\"/content/drive/MyDrive/model_enc_dec.pt\")\n",
        "    input_dim = 10000\n",
        "    hidden_dim = 256\n",
        "    lr = 0.001\n",
        "    encoder = EncoderLSTM(len(ques_words), input_dim, hidden_dim).to(device)\n",
        "    decoder = DecoderLSTM(input_dim, hidden_dim, len(ans_words)).to(device)\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "    encoder.load_state_dict(checkpoint['encoder'])\n",
        "    decoder.load_state_dict(checkpoint['decoder'])\n",
        "    ##encoder_optimizer.load_state_dict(checkpoint['q_optimizer'])\n",
        "    #decoder_optimizer.load_state_dict(checkpoint['a_optimizer'])\n",
        "    print(\"Existing model loaded.\")\n",
        "\n",
        "\n",
        "encoder.load_state_dict(checkpoint['encoder'])\n",
        "decoder.load_state_dict(checkpoint['decoder'])\n",
        "encoder_optimizer.load_state_dict(checkpoint['q_optimizer'])\n",
        "#decoder_optimizer.load_state_dict(checkpoint['a_optimizer'])\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# get some random numbers to choose random sentences\n",
        "#rand_integers = [random.randint(0, 1000) for i in range(1, 20)] #original line that is incorrect\n",
        "rand_integers = [random.randint(0, len(ques_inputs) - 1) for i in range(1, 20)]\n",
        "\n",
        "for i in rand_integers:\n",
        "  h = encoder.init_hidden()\n",
        "  inp = torch.tensor(ques_inputs[i]).unsqueeze(0).to(device)\n",
        "  encoder_outputs, h = encoder(inp, h[0].to(device), h[1].to(device))\n",
        "  print('inp:',inp)\n",
        "  print('en_inputs[',i,']: ', ques_inputs[i])\n",
        "  decoder_input = torch.tensor([ques_w2i['_SOS']]).to(device)\n",
        "  decoder_hidden = h\n",
        "  output = []\n",
        "  attentions = []\n",
        "  while True:\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden[0], decoder_hidden[1])\n",
        "    _, top_index = decoder_output.topk(1)\n",
        "    decoder_input = torch.tensor([top_index.item()]).to(device)\n",
        "    # If the decoder output is the End Of Sentence token, stop decoding process\n",
        "    if top_index.item() == ans_w2i[\"_EOS\"]:\n",
        "      break\n",
        "    output.append(top_index.item())\n",
        "\n",
        "  print(\"Question: \"+ \" \".join([ques_i2w[x] for x in ques_inputs[i]]))\n",
        "  print(\"Answer Predicted: \" + \" \".join([ans_i2w[x] for x in output]))\n",
        "  print(\"Actual: \" + \" \".join([ans_i2w[x] for x in ans_inputs[i]]))\n",
        "  print()\n",
        "\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to test model: ',end-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvZMfMb3o0CW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1bCgZCtgKjD875v_VrH2vWwyU4B7cV4IO",
      "authorship_tag": "ABX9TyNtviLoKHKQUakWETkj4uca",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bff7a571aa1d4a77948d759990ab5e04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e679cf5f6f944e48464ca5a61b62a24",
            "max": 38185,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6ef5e403e174f7b86bfeda545204d3e",
            "value": 38185
          }
        },
        "5e679cf5f6f944e48464ca5a61b62a24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6ef5e403e174f7b86bfeda545204d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff76722aaf044266897e5570d33587f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_618fe8c495264c84994c737f542a6cde",
            "max": 4012060,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5af0419a6eb4c5da0a793d5449d5acf",
            "value": 1965
          }
        },
        "618fe8c495264c84994c737f542a6cde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5af0419a6eb4c5da0a793d5449d5acf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2f3d57bd0194c998ceef72809719a2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_537a17032f4f4f23b03c9e08c1178c07",
            "max": 401206,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a330ca19f9524fff9e4516f389bba209",
            "value": 1965
          }
        },
        "537a17032f4f4f23b03c9e08c1178c07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a330ca19f9524fff9e4516f389bba209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}