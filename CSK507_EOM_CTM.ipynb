{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lumenintellects/seq2seq-chatbot/blob/main/CSK507_EOM_CTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yguDDaQMJZc"
      },
      "source": [
        "#CSK507 End of Module Generative Chatbot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfYarifzMTaV"
      },
      "source": [
        "##1. Import and initialise\n",
        "\n",
        "\n",
        "###1.1 Import\n",
        "First, import everything needed during pre-processing and training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pdDj1ol60WD6"
      },
      "outputs": [],
      "source": [
        "\n",
        "#needed foor comp\n",
        "\n",
        "import datetime\n",
        "overallStart = datetime.datetime.now()\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "import locale\n",
        "print('Prefferred encoding: ',locale.getpreferredencoding())\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "import os #needed to manipulate the downloaded files within the collab environment and to pull data\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import zipfile\n",
        "import spacy.cli\n",
        "from spacy.lang.en import English # updated\n",
        "gpu = spacy.prefer_gpu()\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from ipywidgets import IntProgress\n",
        "from IPython.display import display\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to import all packages: ',end-start)\n",
        "\n",
        "\n",
        "\n",
        "runtype = 'ubuntu' #'ubuntu' or 'wiki'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNs6U_CWNO-3"
      },
      "source": [
        "##1.2 Download and unzip dataset\n",
        "We need to pull the dataset down and unzip from https://www.microsoft.com/en-us/download/confirmation.aspx?id=52419"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UljUgoMO0lza"
      },
      "outputs": [],
      "source": [
        "#https://www.microsoft.com/en-us/download/confirmation.aspx?id=52419\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'wiki':\n",
        "  if os.path.exists('content/WikiQACorpus.zip') == False:\n",
        "    print('zip doesn''t exist, downloading')\n",
        "    !wget -P content https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\n",
        "  else:\n",
        "    print('WikiQACorpus.tsv already exists, not need to download')\n",
        "\n",
        "  if os.path.exists('content/WikiQACorpus/WikiQA.tsv') == False:\n",
        "    print('zip not unzipped, unzipping')\n",
        "    with zipfile.ZipFile('content/WikiQACorpus.zip', 'r') as zip_ref:\n",
        "      zip_ref.extractall('content')\n",
        "  else:\n",
        "    print('WikiQA.tsv already exists, not need to unzip')\n",
        "else:\n",
        "  print('Not running wikiQA corpus')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to download and unzip: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE6WlKLFOB73"
      },
      "source": [
        "##1.3 Pandas DF\n",
        "Put data into pandas df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EQDcAX452DeW"
      },
      "outputs": [],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'wiki':\n",
        "  df = pd.read_csv('content/WikiQACorpus/WikiQA.tsv',on_bad_lines='skip',engine='python',sep='\\t')\n",
        "  end = datetime.datetime.now()\n",
        "  #df = df.loc[0:1000]\n",
        "else:\n",
        "  print('Not running wikiQA corpus')\n",
        "\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to load to df: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMKXnMxkOKWM"
      },
      "source": [
        "#2. Text normalisation, cleaning and extraction\n",
        "\n",
        "##2.1 Normalise using NFKD\n",
        "We first have to nomalise the data using NFKD to make standard alpha so that _é_ becomes _e_ etc. Additionally, check the coding by converting to ascii and back to UTF-8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R1f_dU24pqM"
      },
      "outputs": [],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'wiki':\n",
        "  df['Question'] = df.Question.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') # this will make the text normal alpha, for exaple é will be e, etc\n",
        "  df['Sentence'] = df.Sentence.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') # this will make the text normal alpha, for exaple é will be e, etc\n",
        "else :\n",
        "  print('Not running wikiQA corpus')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to nomalise: ',end-start)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEsTE1HHTZMR"
      },
      "source": [
        "##2.2 Clean data\n",
        "Next we clean the data by removing triple and double spaces befre making lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOvIBlzEQOMF"
      },
      "outputs": [],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'wiki':\n",
        "  df['Question'] = df['Question'].str.replace('\\n',' ') # remove carriage returns\n",
        "  df['Question'] = df['Question'].str.replace('   ',' ') #remove triple spaces\n",
        "  df['Question'] = df['Question'].str.replace('  ',' ') # also remove double sapces (which might remain after triples are removed)\n",
        "  df['Question'] = df['Question'].str.lower()\n",
        "  df['Sentence'] = df['Sentence'].str.replace('\\n',' ') # remove carriage returns\n",
        "  df['Sentence'] = df['Sentence'].str.replace('   ',' ') #remove triple spaces\n",
        "  df['Sentence'] = df['Sentence'].str.replace('  ',' ') # also rmeove double sapces (which might remain after triples are removed)\n",
        "  df['Sentence'] = df['Sentence'].str.lower()\n",
        "else :\n",
        "  print('Not running wikiQA corpus')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to clean: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10V8qcFwU1Ku"
      },
      "source": [
        "##2.3 Data Extraction\n",
        "Next we extract the relevant normalised and cleansed columns of dataframe into ques and ans dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLJBIEVYPrGk"
      },
      "outputs": [],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'wiki':\n",
        "  ques = df['Question']\n",
        "  ans = df['Sentence']\n",
        "else :\n",
        "  print('Not running wikiQA corpus')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to extract: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf2X0lPpkBIA"
      },
      "source": [
        "#2.4 Export to csv for team wide review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoBg9_eSh7Jk"
      },
      "outputs": [],
      "source": [
        "if runtype == 'wiki':\n",
        "  outputdf = pd.DataFrame({'input': ques, 'output': ans})\n",
        "  outputdf.to_csv('outputdf.csv')\n",
        "else :\n",
        "  print('Not running wikiQA corpus')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9gdA7VurWCe"
      },
      "source": [
        "##2.5 Switch to Ubuntu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUKgXikirekt"
      },
      "outputs": [],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if runtype == 'ubuntu':\n",
        "  dfu = pd.read_csv('/content/drive/MyDrive/ubuntu_dialogue_corpus_input_output_pairs.csv',on_bad_lines='skip',engine='python')\n",
        "\n",
        "\n",
        "\n",
        "  print('length of df before splitting:', len(dfu))\n",
        "\n",
        "  first_PCT = int(len(dfu)*0.005)\n",
        "  df = dfu.loc[0:first_PCT]\n",
        "\n",
        "  print('length of df after splitting:', len(df))\n",
        "\n",
        "\n",
        "  ques = df['input']\n",
        "  ans = df['output']\n",
        "  df = pd.DataFrame({'input': ques, 'output': ans})\n",
        "else :\n",
        "  print('Not running ubuntu corpus')\n",
        "\n",
        "#df = df.loc[0:100]\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to load to df: ',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-6Ka8UgXtPC"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDSH1m9UVLQH"
      },
      "source": [
        "#3. Tokenization and indexing\n",
        "\n",
        "Next we need to tokenize the data using spaCy before making word-to-index and index-to-word mappings. Finally we will need to convert toindices for the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "544SRqLvTr6Y"
      },
      "source": [
        "##3.1 Import spaCy Pipeline\n",
        "\n",
        "Large Class used because of copus complexity, en_core_web_lg contains 343k unique vectors, compared with en_core_web_sm which only has 20k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEx2LiG5L5vA"
      },
      "outputs": [],
      "source": [
        "\n",
        "#needed foor comp\n",
        "\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "#because of the complex corpus, use the larger en_core_web_lg\n",
        "\n",
        "try:\n",
        "  nlp = spacy.load(\"en_core_web_lg\")\n",
        "except OSError:\n",
        "  print('Downloading en_core_web_lg')\n",
        "  spacy.cli.download(\"en_core_web_lg\")\n",
        "  nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to download and load spaCy class: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urh0je79WRTY"
      },
      "source": [
        "##3.2 Counters\n",
        "\n",
        "We need a way of getting and storing the frequency, so we need to initalise counters (from collections package)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpaE6JYlWQMn"
      },
      "outputs": [],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ques_words = Counter()\n",
        "ans_words = Counter()\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to initialise counters: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRQPxKaDWyLE"
      },
      "source": [
        "##3.3 Tokenize\n",
        "\n",
        "totokenize, we use spaCy pieline already loaded and go through the entire df row by row before stitching _EOS onto the end for the seq-to-seq model later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLJPx5n6XTey"
      },
      "outputs": [],
      "source": [
        "#start = datetime.datetime.now()\n",
        "#print(start)\n",
        "#longString = ' '.join(df.input.astype(str).tolist()) #get all comments into string\n",
        "#print('cp1')\n",
        "#lowerString = ' '.join(longString.lower().split())\n",
        "#print('cp2 ')\n",
        "#words = lowerString.split()\n",
        "#print('cp3')\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyKKtpdab1sg"
      },
      "outputs": [],
      "source": [
        "##print(longString)\n",
        "#len(longString)\n",
        "#dfw = pd.DataFrame({'word': list(words)})\n",
        "#dfw.drop_duplicates(inplace=True)\n",
        "#dfw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuUHSslBwN7h"
      },
      "outputs": [],
      "source": [
        "#ques = ques.apply(lambda x: str(x) + ' _EOS')\n",
        "#ques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7PHI-21bVMy"
      },
      "outputs": [],
      "source": [
        "#print(dfw)\n",
        "#terms = np.unique(words)\n",
        "#print('cp4')\n",
        "#\n",
        "#terms = np.sort(terms, axis=None)\n",
        "#end = datetime.datetime.now()\n",
        "#print(end)\n",
        "#\n",
        "#print('Time taken to get BoW: ',end-start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv02xT7m5IK7"
      },
      "outputs": [],
      "source": [
        "\n",
        "#needed foor comp\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "# Check if the CSV files already exist\n",
        "csv_files_exist = all([\n",
        "    os.path.exists('/content/drive/MyDrive/ques_inputs.csv'),\n",
        "    os.path.exists('/content/drive/MyDrive/ans_inputs.csv'),\n",
        "    os.path.exists('/content/drive/MyDrive/ques_words.csv'),\n",
        "    os.path.exists('/content/drive/MyDrive/ans_words.csv')\n",
        "])\n",
        "\n",
        "if csv_files_exist:\n",
        "    print(\"CSV files already exist. Skipping tokenization.\")\n",
        "    # Load the data from the CSV files\n",
        "    ques_inputs = pd.read_csv('/content/drive/MyDrive/ques_inputs.csv').values.tolist()\n",
        "    ans_inputs = pd.read_csv('/content/drive/MyDrive/ans_inputs.csv').values.tolist()\n",
        "    ques_words = pd.read_csv('/content/drive/MyDrive/ques_words.csv').values.tolist()\n",
        "    ans_words = pd.read_csv('/content/drive/MyDrive/ans_words.csv').values.tolist()\n",
        "else:\n",
        "    print(\"CSV files not found. Starting tokenization.\")\n",
        "    gpu = spacy.prefer_gpu() #GPU was found to be slightly better peforming in colab for SpaCy. The GPU selection needs to be immediately before the load command so need to reload\n",
        "    nlp = spacy.load(\"en_core_web_lg\") # large was used\n",
        "\n",
        "    ques_inputs = []\n",
        "    ans_inputs = []\n",
        "\n",
        "    f = IntProgress(min=0, max=len(df)) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "    display(f) # display the bar\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        ques_tokens = nlp(str(ques[i]))\n",
        "        ans_tokens = nlp(str(ans[i]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if (len(ques_tokens)!=0 and len(ans_tokens)!=0):\n",
        "\n",
        "\n",
        "          for token in ques_tokens:\n",
        "              ques_words.update([token.text]) #this is the counter for the question frequency, update it\n",
        "\n",
        "\n",
        "          ques_inputs.append([token.text for token in ques_tokens] + ['_EOS'])\n",
        "\n",
        "          for token in ans_tokens:\n",
        "                ans_words.update([token.text]) #this is the counter for the answer frequency, update it\n",
        "\n",
        "          ans_inputs.append([token.text for token in ans_tokens] + ['_EOS'])\n",
        "        f.value += 1\n",
        "    pd.DataFrame(ques_inputs).to_csv('/content/drive/MyDrive/ques_inputs.csv', index=False)\n",
        "    pd.DataFrame(ans_inputs).to_csv('/content/drive/MyDrive/ans_inputs.csv', index=False)\n",
        "    pd.DataFrame(ques_words).to_csv('/content/drive/MyDrive/ques_words.csv', index=False)\n",
        "    pd.DataFrame(ans_words).to_csv('/content/drive/MyDrive/ans_words.csv', index=False)\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "for i in range(len(ques_inputs)):\n",
        "    ques_inputs[i] = [int(x) for x in ques_inputs[i] if x != '' and not pd.isnull(x)]\n",
        "\n",
        "for i in range(len(ans_inputs)):\n",
        "    ans_inputs[i] = [int(x) for x in ans_inputs[i] if x != '' and not pd.isnull(x)]\n",
        "\n",
        "print(ques_inputs[1])\n",
        "print(ans_inputs[1])\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to tokenize: ',end-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTRXje7a0zU"
      },
      "source": [
        "##3.4 Vocabulary creation\n",
        "\n",
        "Next we need to create word-to-index and index-to-word for both questions and answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONeEQ3nirM0q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLbyd_Q8avs1"
      },
      "outputs": [],
      "source": [
        "\n",
        "#needed foor comp\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "\n",
        "#ques_words = ['_SOS','_EOS','_UNK'] + sorted(ques_words,key=ques_words.get,reverse=True) #add 'special words' like EOS to the entire word list\n",
        "#ans_words = ['_SOS','_EOS','_UNK'] + sorted(ans_words,key=ans_words.get,reverse=True)\n",
        "\n",
        "ques_words = [item for sublist in ques_words for item in sublist]\n",
        "ans_words = [item for sublist in ans_words for item in sublist]\n",
        "\n",
        "ques_w2i = {a:i for i,a in enumerate(ques_words)} #record the index against each word with the word first in a dictionary\n",
        "ans_w2i = {a:i for i,a in enumerate(ans_words)}\n",
        "\n",
        "\n",
        "ques_i2w = {i:o for i,o in enumerate(ques_words)} #do the same thing but in reverse so the index is first\n",
        "ans_i2w = {i:o for i,o in enumerate(ans_words)}\n",
        "\n",
        "\n",
        "\n",
        "#for i in range(len(ques_inputs)):\n",
        "if len(ques_inputs) >0 :\n",
        "  print('no need to repop inputs')\n",
        "else :\n",
        "  g = IntProgress(min=0, max=len(ques_inputs)) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "  display(g) # display the bar\n",
        "  for i in range(len(ans_inputs)):\n",
        "          ques_sentence = str(ques_inputs[i])\n",
        "          ans_sentence = str(ans_inputs[i])\n",
        "          ques_inputs[i] = [ques_w2i.get(word, ques_w2i['_UNK']) for word in ques_inputs[i]]  # Use get to handle missing words, replace with _UNK token\n",
        "          ans_inputs[i] = [ans_w2i.get(word, ans_w2i['_UNK']) for word in ans_inputs[i]]  # Use get to handle missing words, replace with _UNK token\n",
        "          g.value += 1\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to create vocabulary: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuSh-YJ2gzXB"
      },
      "source": [
        "#lifted straight from CSK507 coursework:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imfEpfxeDt47"
      },
      "outputs": [],
      "source": [
        "\n",
        "#needed foor comp\n",
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, vocab_len, input_dim, hidden_dim, n_layers, drop_prob=0):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_len, input_dim)\n",
        "    self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers,\n",
        "                        dropout=drop_prob, batch_first=True)\n",
        "\n",
        "  def forward(self, inputs, encoder_state_vector, encoder_cell_vector):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embedded = self.embedding(inputs).to(device)\n",
        "    # Pass the embedded word vectors into LSTM and return all outputs\n",
        "    output, hidden = self.lstm(embedded, (encoder_state_vector, encoder_cell_vector))\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size=1):\n",
        "    return (torch.zeros(self.n_layers, batch_size,\n",
        "                        self.hidden_dim),\n",
        "            torch.zeros(self.n_layers, batch_size,\n",
        "                        self.hidden_dim))\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_vocab_len, n_layers, drop_prob=0.1):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_vocab_len = output_vocab_len\n",
        "    self.n_layers = n_layers\n",
        "    self.drop_prob = drop_prob\n",
        "    self.input_dim = input_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(self.output_vocab_len, self.input_dim)\n",
        "    self.dropout = nn.Dropout(self.drop_prob)\n",
        "    self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, batch_first=True)\n",
        "    self.classifier = nn.Linear(self.hidden_dim, self.output_vocab_len)\n",
        "\n",
        "  def forward(self, inputs, decoder_state_vector, decoder_context_vector):\n",
        "    # Embed input words\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embedded = self.embedding(inputs).view(1, -1).to(device)\n",
        "    embedded = self.dropout(embedded)\n",
        "    embedded = embedded.unsqueeze(0)\n",
        "\n",
        "    output, hidden = self.lstm(embedded, (decoder_state_vector,\n",
        "                                          decoder_context_vector))\n",
        "\n",
        "    # Pass LSTM outputs through a Linear layer acting as a classifier\n",
        "    output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
        "\n",
        "    return output, hidden\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_w-5Xuxg8uC"
      },
      "outputs": [],
      "source": [
        "\n",
        "#needed foor comp\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(start)\n",
        "print()\n",
        "#i=0\n",
        "input_dim = len(ques_words)\n",
        "hidden_dim = 512\n",
        "n_layers = 1\n",
        "spacy.prefer_gpu()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = EncoderLSTM(len(ques_words), input_dim, hidden_dim, n_layers).to(device)\n",
        "decoder = DecoderLSTM(input_dim, hidden_dim, len(ans_words),n_layers).to(device)\n",
        "\n",
        "lr = 0.001\n",
        "#encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "#decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "if os.path.exists(\"/content/drive/MyDrive/model_enc_dec.pt\"):\n",
        "    # Load the existing model\n",
        "    checkpoint = torch.load(\"/content/drive/MyDrive/model_enc_dec.pt\", map_location=torch.device(device))\n",
        "    encoder.load_state_dict(checkpoint['encoder'])\n",
        "    decoder.load_state_dict(checkpoint['decoder'])\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "    print(\"Existing model loaded.\")\n",
        "else:\n",
        "    # Create a new model\n",
        "    encoder = EncoderLSTM(len(ques_words), input_dim, hidden_dim, n_layers).to(device)\n",
        "    decoder = DecoderLSTM(input_dim, hidden_dim, len(ans_words), n_layers).to(device)\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "    print(\"New model created.\")\n",
        "\n",
        "EPOCHS = 1\n",
        "teacher_forcing_prob = 0.5\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "tk0 = range(1,EPOCHS+1)\n",
        "tk2 = enumerate(ques_inputs)\n",
        "max_index = 0\n",
        "for b, sentence in tk2:\n",
        "  for bb in range(len(ans_inputs[b])):\n",
        "    max_index += 1\n",
        "\n",
        "k = IntProgress(min=0, max=max_index * EPOCHS) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "print('overall training progress')\n",
        "print()\n",
        "display(k)\n",
        "print()\n",
        "for epoch in tk0:\n",
        "    avg_loss = 0.\n",
        "    j = IntProgress(min=0, max=max_index) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "    print('epoch ', epoch)\n",
        "    display(j) # display the bar\n",
        "    print()\n",
        "    tk1 = enumerate(ques_inputs)\n",
        "    for i, sentence in tk1:\n",
        "        #print(i)\n",
        "        #print(sentence)\n",
        "        loss = 0.\n",
        "\n",
        "        #initialise encoder state vector and cell state vector\n",
        "        h = encoder.init_hidden()\n",
        "        encoder_state_vector = h[0]\n",
        "        encoder_state_vector = encoder_state_vector.to(device)\n",
        "        encoder_cell_vector = h[0]\n",
        "        encoder_cell_vector = encoder_cell_vector.to(device)\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        inp = torch.tensor(sentence).unsqueeze(0).to(device)\n",
        "\n",
        "        #print('inp: ', epoch, inp)\n",
        "        #if (i % 1000) == 0:\n",
        "        #  print('inp: ', i, epoch)\n",
        "        encoder_outputs, h = encoder(inp, encoder_state_vector, encoder_cell_vector)\n",
        "\n",
        "        #First decoder input will be the SOS token\n",
        "        decoder_input = torch.tensor([ques_w2i['_SOS']]).to(device)\n",
        "        #First decoder hidden state will be last encoder hidden state\n",
        "        decoder_hidden = h\n",
        "\n",
        "        output = []\n",
        "        teacher_forcing = True if random.random() < teacher_forcing_prob else False\n",
        "        #k = IntProgress(min=0, max=len(ans_inputs[i])) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "        #k.value=0\n",
        "        #display(k) # display the bar\n",
        "\n",
        "        for ii in range(len(ans_inputs[i])):\n",
        "          decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden[0], decoder_hidden[1])\n",
        "\n",
        "          # Get the index value of the word with the highest score from the decoder output\n",
        "          top_value, top_index = decoder_output.topk(1)\n",
        "          if teacher_forcing:\n",
        "            decoder_input = torch.tensor([ans_inputs[i][ii]]).to(device)\n",
        "          else:\n",
        "            decoder_input = torch.tensor([top_index.item()]).to(device)\n",
        "\n",
        "          output.append(top_index.item())\n",
        "          # Calculate the loss of the prediction against the actual word\n",
        "          loss += F.nll_loss(decoder_output.view(1,-1), torch.tensor([ans_inputs[i][ii]]).to(device))\n",
        "          j.value += 1\n",
        "          k.value += 1\n",
        "\n",
        "          #print(j.value)\n",
        "          #print((j.value/(len(ans_inputs) * max_index))*100)\n",
        "\n",
        "        loss.backward()\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "        avg_loss += loss.item()/len(ques_inputs)\n",
        "    print(avg_loss)\n",
        "    print(' epoch complete at: ',datetime.datetime.now())\n",
        "    torch.save({\"encoder\":encoder.state_dict(),\"decoder\":decoder.state_dict(),\"q_optimizer\":encoder_optimizer.state_dict(),\"a_optimizer\":decoder_optimizer},\"/content/drive/MyDrive/model_enc_dec.pt\")\n",
        "\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to train model: ',end-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7asNohDkPkG"
      },
      "outputs": [],
      "source": [
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device =\"cpu\"\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/model_enc_dec.pt\", map_location=torch.device(device))\n",
        "\n",
        "if os.path.exists(\"/content/drive/MyDrive/model_enc_dec.pt\"):\n",
        "    # Load the existing model\n",
        "    #checkpoint = torch.load(\"/content/drive/MyDrive/model_enc_dec.pt\")\n",
        "    input_dim = len(ques_words)\n",
        "    hidden_dim = 512\n",
        "    n_layers = 1\n",
        "    lr = 0.001\n",
        "    encoder = EncoderLSTM(len(ques_words), input_dim, hidden_dim,n_layers).to(device)\n",
        "    decoder = DecoderLSTM(input_dim, hidden_dim, len(ans_words),n_layers).to(device)\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "    encoder.load_state_dict(checkpoint['encoder'])\n",
        "    decoder.load_state_dict(checkpoint['decoder'])\n",
        "    ##encoder_optimizer.load_state_dict(checkpoint['q_optimizer'])\n",
        "    #decoder_optimizer.load_state_dict(checkpoint['a_optimizer'])\n",
        "    print(\"Existing model loaded.\")\n",
        "\n",
        "\n",
        "encoder.load_state_dict(checkpoint['encoder'])\n",
        "decoder.load_state_dict(checkpoint['decoder'])\n",
        "encoder_optimizer.load_state_dict(checkpoint['q_optimizer'])\n",
        "#decoder_optimizer.load_state_dict(checkpoint['a_optimizer'])\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# get some random numbers to choose random sentences\n",
        "#rand_integers = [random.randint(0, 1000) for i in range(1, 20)] #original line that is incorrect\n",
        "rand_integers = [random.randint(0, len(ques_inputs) - 1) for i in range(1, 20)]\n",
        "\n",
        "for i in rand_integers:\n",
        "  h = encoder.init_hidden()\n",
        "  inp = torch.tensor(ques_inputs[i]).unsqueeze(0).to(device)\n",
        "  encoder_outputs, h = encoder(inp, h[0].to(device), h[1].to(device))\n",
        "  print('input vector:',inp)\n",
        "  print('inputs[',i,']: ', ques_inputs[i])\n",
        "  decoder_input = torch.tensor([ques_w2i['_SOS']]).to(device)\n",
        "  decoder_hidden = h\n",
        "  output = []\n",
        "  attentions = []\n",
        "  while True:\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden[0], decoder_hidden[1])\n",
        "    _, top_index = decoder_output.topk(1)\n",
        "    decoder_input = torch.tensor([top_index.item()]).to(device)\n",
        "    # If the decoder output is the End Of Sentence token, stop decoding process\n",
        "    if top_index.item() == ans_w2i[\"_EOS\"]:\n",
        "      break\n",
        "    output.append(top_index.item())\n",
        "\n",
        "  print(\"Question: \"+ \" \".join([ques_i2w[x] for x in ques_inputs[i]]))\n",
        "  print(\"Answer Predicted: \" + \" \".join([ans_i2w[x] for x in output]))\n",
        "  print(\"Actual: \" + \" \".join([ans_i2w[x] for x in ans_inputs[i]]))\n",
        "  print()\n",
        "\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to test model: ',end-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR6uRPgsusfa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1bCgZCtgKjD875v_VrH2vWwyU4B7cV4IO",
      "authorship_tag": "ABX9TyP2uvkLhrPYy7ftkTtX2C0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}