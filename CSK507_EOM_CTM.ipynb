{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO7bxW8yurapZPT+k6BjqDJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a8944491f0a346978a7d1f9101cc24e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8a578b69f1f48b2909e0bcbddbf4ba7",
            "max": 101155,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a25d59c873b44488cbd424b0da0dfa5",
            "value": 1418
          }
        },
        "f8a578b69f1f48b2909e0bcbddbf4ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a25d59c873b44488cbd424b0da0dfa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lumenintellects/seq2seq-chatbot/blob/main/CSK507_EOM_CTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CSK507 End of Module Generative Chatbot\n"
      ],
      "metadata": {
        "id": "6yguDDaQMJZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Import and initialise\n",
        "\n",
        "\n",
        "###1.1 Import\n",
        "First, import everything needed during pre-processing and training\n"
      ],
      "metadata": {
        "id": "jfYarifzMTaV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdDj1ol60WD6",
        "outputId": "026fb68a-9125-4210-cb85-300c34734e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:15:30.433334\n",
            "Prefferred encoding:  UTF-8\n",
            "2024-12-06 12:15:39.318806\n",
            "Time taken to import all packages:  0:00:08.885472\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "overallStart = datetime.datetime.now()\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "import locale\n",
        "print('Prefferred encoding: ',locale.getpreferredencoding())\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "import os #needed to manipulate the downloaded files within the collab environment and to pull data\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import zipfile\n",
        "import spacy.cli\n",
        "from spacy.lang.en import English # updated\n",
        "gpu = spacy.prefer_gpu()\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from ipywidgets import IntProgress\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to import all packages: ',end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Download and unzip dataset\n",
        "We need to pull the dataset down and unzip from https://www.microsoft.com/en-us/download/confirmation.aspx?id=52419"
      ],
      "metadata": {
        "id": "GNs6U_CWNO-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.microsoft.com/en-us/download/confirmation.aspx?id=52419\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if os.path.exists('content/WikiQACorpus.zip') == False:\n",
        "  print('zip doesn''t exist, downloading')\n",
        "  !wget -P content https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\n",
        "else:\n",
        "  print('WikiQACorpus.tsv already exists, not need to download')\n",
        "\n",
        "if os.path.exists('content/WikiQACorpus/WikiQA.tsv') == False:\n",
        "  print('zip not unzipped, unzipping')\n",
        "  with zipfile.ZipFile('content/WikiQACorpus.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('content')\n",
        "else:\n",
        "  print('WikiQA.tsv already exists, not need to unzip')\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to download and unzip: ',end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UljUgoMO0lza",
        "outputId": "589addd8-fc8f-4179-9bfa-169ccf35a711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:15:39.329283\n",
            "zip doesnt exist, downloading\n",
            "--2024-12-06 12:15:39--  https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 23.54.42.9, 2600:1407:3c00:10a1::317f, 2600:1407:3c00:108a::317f\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|23.54.42.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7094233 (6.8M) [application/octet-stream]\n",
            "Saving to: ‘content/WikiQACorpus.zip’\n",
            "\n",
            "WikiQACorpus.zip    100%[===================>]   6.76M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-12-06 12:15:39 (63.1 MB/s) - ‘content/WikiQACorpus.zip’ saved [7094233/7094233]\n",
            "\n",
            "zip not unzipped, unzipping\n",
            "2024-12-06 12:15:39.771767\n",
            "Time taken to download and unzip:  0:00:00.442484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3 Pandas DF\n",
        "Put data into pandas df"
      ],
      "metadata": {
        "id": "wE6WlKLFOB73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "df = pd.read_csv('content/WikiQACorpus/WikiQA.tsv',on_bad_lines='skip',engine='python',sep='\\t')\n",
        "end = datetime.datetime.now()\n",
        "#df = df.loc[0:1000]\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to load to df: ',end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQDcAX452DeW",
        "outputId": "1ee0d384-3a6b-4424-a289-a5767b16aa3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:15:39.779628\n",
            "2024-12-06 12:15:39.985378\n",
            "Time taken to load to df:  0:00:00.205750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Text normalisation, cleaning and extraction\n",
        "\n",
        "##2.1 Normalise using NFKD\n",
        "We first have to nomalise the data using NFKD to make standard alpha so that _é_ becomes _e_ etc. Additionally, check the coding by converting to ascii and back to UTF-8."
      ],
      "metadata": {
        "id": "CMKXnMxkOKWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "df['Question'] = df.Question.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') # this will make the text normal alpha, for exaple é will be e, etc\n",
        "df['Sentence'] = df.Sentence.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') # this will make the text normal alpha, for exaple é will be e, etc\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to nomalise: ',end-start)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R1f_dU24pqM",
        "outputId": "5c054e12-e39d-44d4-f601-bdaba1fe9d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:15:39.992947\n",
            "2024-12-06 12:15:40.069758\n",
            "Time taken to nomalise:  0:00:00.076811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 Clean data\n",
        "Next we clean the data by removing triple and double spaces befre making lower case"
      ],
      "metadata": {
        "id": "YEsTE1HHTZMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "df['Question'] = df['Question'].str.replace('\\n',' ') # remove carriage returns\n",
        "df['Question'] = df['Question'].str.replace('   ',' ') #remove triple spaces\n",
        "df['Question'] = df['Question'].str.replace('  ',' ') # also remove double sapces (which might remain after triples are removed)\n",
        "df['Question'] = df['Question'].str.lower()\n",
        "df['Sentence'] = df['Sentence'].str.replace('\\n',' ') # remove carriage returns\n",
        "df['Sentence'] = df['Sentence'].str.replace('   ',' ') #remove triple spaces\n",
        "df['Sentence'] = df['Sentence'].str.replace('  ',' ') # also rmeove double sapces (which might remain after triples are removed)\n",
        "df['Sentence'] = df['Sentence'].str.lower()\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to clean: ',end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOvIBlzEQOMF",
        "outputId": "46a5cbd0-14e2-428e-e909-f55a18de0b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:15:40.078671\n",
            "2024-12-06 12:15:40.147352\n",
            "Time taken to clean:  0:00:00.068681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.3 Data Extraction\n",
        "Next we extract the relevant normalised and cleansed columns of dataframe into ques and ans dataframes"
      ],
      "metadata": {
        "id": "10V8qcFwU1Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "\n",
        "ques = df['Question']\n",
        "ans = df['Sentence']\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to extract: ',end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLJBIEVYPrGk",
        "outputId": "05703075-702e-4b45-b594-f19aedf3100f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:15:40.153427\n",
            "2024-12-06 12:15:40.153709\n",
            "Time taken to extract:  0:00:00.000282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.4 Export to csv for team wide review"
      ],
      "metadata": {
        "id": "rf2X0lPpkBIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputdf = pd.DataFrame({'input': ques, 'output': ans})\n",
        "outputdf.to_csv('outputdf.csv')"
      ],
      "metadata": {
        "id": "MoBg9_eSh7Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.5 Switch to Ubuntu"
      ],
      "metadata": {
        "id": "g9gdA7VurWCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "dfu = pd.read_csv('ubuntu_dialogue_corpus_input_output_pairs.csv',on_bad_lines='skip',engine='python')\n",
        "\n",
        "\n",
        "ques = dfu['input']\n",
        "ans = dfu['output']\n",
        "\n",
        "df = pd.DataFrame({'input': ques1, 'output': ans1})\n",
        "first_20PCT = int(len(df)*0.2)\n",
        "df = df.loc[0:first_20PCT]\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to load to df: ',end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUKgXikirekt",
        "outputId": "abcd0709-c631-4980-c373-25ef8335673d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:54:56.214556\n",
            "2024-12-06 12:55:13.689842\n",
            "Time taken to load to df:  0:00:17.475286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF1ynlWe1jUT",
        "outputId": "0a0bebf8-ba93-464b-bb6e-305bc7d920b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101155"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Tokenization and indexing\n",
        "\n",
        "Next we need to tokenize the data using spaCy before making word-to-index and index-to-word mappings. Finally we will need to convert toindices for the model"
      ],
      "metadata": {
        "id": "rDSH1m9UVLQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Import spaCy Pipeline\n",
        "\n",
        "Large Class used because of copus complexity, en_core_web_lg contains 343k unique vectors, compared with en_core_web_sm which only has 20k"
      ],
      "metadata": {
        "id": "544SRqLvTr6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "#because of the complex corpus, use the larger en_core_web_lg\n",
        "\n",
        "try:\n",
        "  nlp = spacy.load(\"en_core_web_lg\")\n",
        "except OSError:\n",
        "  print('Downloading en_core_web_lg')\n",
        "  spacy.cli.download(\"en_core_web_lg\")\n",
        "  nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to download and load spaCy class: ',end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEx2LiG5L5vA",
        "outputId": "5fe658de-4d49-4d24-bad3-d86f8a5d7b3c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:55:13.704457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:55:15.433071\n",
            "Time taken to download and load spaCy class:  0:00:01.728614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Counters\n",
        "\n",
        "We need a way of getting and storing the frequency, so we need to initalise counters (from collections package)"
      ],
      "metadata": {
        "id": "Urh0je79WRTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ques_words = Counter()\n",
        "ans_words = Counter()\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to initialise counters: ',end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpaE6JYlWQMn",
        "outputId": "a134f789-82d8-4876-ee2d-d08c236ce8df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:55:15.440883\n",
            "2024-12-06 12:55:15.441284\n",
            "Time taken to initialise counters:  0:00:00.000401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Tokenize\n",
        "\n",
        "totokenize, we use spaCy pieline already loaded and go through the entire df row by row before stitching _EOS onto the end for the seq-to-seq model later"
      ],
      "metadata": {
        "id": "jRQPxKaDWyLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "\n",
        "gpu = spacy.prefer_gpu() #GPU was found to be slightly better peforming in colab for SpaCy. The GPU selection needs to be immediately before the load command so need to reload\n",
        "nlp = spacy.load(\"en_core_web_lg\") # large was used\n",
        "\n",
        "ques_inputs = []\n",
        "ans_inputs = []\n",
        "\n",
        "f = IntProgress(min=0, max=len(df)) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "display(f) # display the bar\n",
        "\n",
        "for i in range(len(df)):\n",
        "    ques_tokens = nlp(str(ques[i]))\n",
        "    ans_tokens = nlp(str(ans[i]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if (len(ques_tokens)!=0 and len(ans_tokens)!=0):\n",
        "\n",
        "\n",
        "      for token in ques_tokens:\n",
        "          ques_words.update([token.text]) #this is the counter for the question frequency, update it\n",
        "\n",
        "\n",
        "      ques_inputs.append([token.text for token in ques_tokens] + ['_EOS'])\n",
        "\n",
        "      for token in ans_tokens:\n",
        "          ans_words.update([token.text]) #this is the counter for the answer frequency, update it\n",
        "\n",
        "      ans_inputs.append([token.text for token in ans_tokens] + ['_EOS'])\n",
        "    f.value += 1\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to tokenize: ',end-start)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a8944491f0a346978a7d1f9101cc24e4",
            "f8a578b69f1f48b2909e0bcbddbf4ba7",
            "1a25d59c873b44488cbd424b0da0dfa5"
          ]
        },
        "id": "sv02xT7m5IK7",
        "outputId": "86f8e868-e530-4e71-9ff3-24c84e1f9667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-06 12:55:15.457433\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntProgress(value=0, max=101155)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8944491f0a346978a7d1f9101cc24e4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.4 Vocabulary creation\n",
        "\n",
        "Next we need to create word-to-index and index-to-word for both questions and answers"
      ],
      "metadata": {
        "id": "2yTRXje7a0zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "\n",
        "ques_words = ['_SOS','_EOS','_UNK'] + sorted(ques_words,key=ques_words.get,reverse=True) #add 'special words' like EOS to the entire word list\n",
        "ans_words = ['_SOS','_EOS','_UNK'] + sorted(ans_words,key=ans_words.get,reverse=True)\n",
        "\n",
        "\n",
        "ques_w2i = {a:i for i,a in enumerate(ques_words)} #record the index against each word with the word first in a dictionary\n",
        "ans_w2i = {a:i for i,a in enumerate(ans_words)}\n",
        "\n",
        "\n",
        "ques_i2w = {i:o for i,o in enumerate(ques_words)} #do the same thing but in reverse so the index is first\n",
        "ans_i2w = {i:o for i,o in enumerate(ans_words)}\n",
        "\n",
        "g = IntProgress(min=0, max=len(ques_inputs)) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "display(g) # display the bar\n",
        "\n",
        "for i in range(len(ques_inputs)):\n",
        "    ques_sentence = ques_inputs[i]\n",
        "    ans_sentence = ans_inputs[i]\n",
        "    ques_inputs[i] = [ques_w2i[word] for word in ques_sentence] #this puts columns accross the df to store frequency of each word\n",
        "    ans_inputs[i] = [ans_w2i[word] for word in ans_sentence]\n",
        "    g.value += 1\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to create vocabulary: ',end-start)"
      ],
      "metadata": {
        "id": "hLbyd_Q8avs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lifted straight from CSK507 coursework:"
      ],
      "metadata": {
        "id": "vuSh-YJ2gzXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLSTM(nn.Module):\n",
        "  def __init__(self, vocab_len, input_dim, hidden_dim, n_layers=1, drop_prob=0):\n",
        "    super(EncoderLSTM, self).__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_len, input_dim)\n",
        "    self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers,\n",
        "                        dropout=drop_prob, batch_first=True)\n",
        "\n",
        "  def forward(self, inputs, encoder_state_vector, encoder_cell_vector):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embedded = self.embedding(inputs).to(device)\n",
        "    # Pass the embedded word vectors into LSTM and return all outputs\n",
        "    output, hidden = self.lstm(embedded, (encoder_state_vector, encoder_cell_vector))\n",
        "    return output, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size=1):\n",
        "    return (torch.zeros(self.n_layers, batch_size,\n",
        "                        self.hidden_dim),\n",
        "            torch.zeros(self.n_layers, batch_size,\n",
        "                        self.hidden_dim))\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_vocab_len, n_layers=1, drop_prob=0.1):\n",
        "    super(DecoderLSTM, self).__init__()\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_vocab_len = output_vocab_len\n",
        "    self.n_layers = n_layers\n",
        "    self.drop_prob = drop_prob\n",
        "    self.input_dim = input_dim\n",
        "\n",
        "    self.embedding = nn.Embedding(self.output_vocab_len, self.input_dim)\n",
        "    self.dropout = nn.Dropout(self.drop_prob)\n",
        "    self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, batch_first=True)\n",
        "    self.classifier = nn.Linear(self.hidden_dim, self.output_vocab_len)\n",
        "\n",
        "  def forward(self, inputs, decoder_state_vector, decoder_context_vector):\n",
        "    # Embed input words\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    embedded = self.embedding(inputs).view(1, -1).to(device)\n",
        "    embedded = self.dropout(embedded)\n",
        "    embedded = embedded.unsqueeze(0)\n",
        "\n",
        "    output, hidden = self.lstm(embedded, (decoder_state_vector,\n",
        "                                          decoder_context_vector))\n",
        "\n",
        "    # Pass LSTM outputs through a Linear layer acting as a classifier\n",
        "    output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
        "\n",
        "    return output, hidden\n",
        "\n"
      ],
      "metadata": {
        "id": "imfEpfxeDt47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start = datetime.datetime.now()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(start)\n",
        "print()\n",
        "#i=0\n",
        "input_dim = 100\n",
        "hidden_dim = 256\n",
        "spacy.prefer_gpu()\n",
        "encoder = EncoderLSTM(len(ques_words), input_dim, hidden_dim).to('cuda')\n",
        "decoder = DecoderLSTM(input_dim, hidden_dim, len(ans_words)).to('cuda')\n",
        "\n",
        "lr = 0.001\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
        "\n",
        "EPOCHS = 10\n",
        "teacher_forcing_prob = 0.5\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "tk0 = range(1,EPOCHS+1)\n",
        "tk2 = enumerate(ques_inputs)\n",
        "max_index = 0\n",
        "for b, sentence in tk2:\n",
        "  for bb in range(len(ans_inputs[b])):\n",
        "    max_index += 1\n",
        "\n",
        "k = IntProgress(min=0, max=max_index * EPOCHS) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "print('overall training progress')\n",
        "print()\n",
        "display(k)\n",
        "print()\n",
        "for epoch in tk0:\n",
        "    avg_loss = 0.\n",
        "    j = IntProgress(min=0, max=max_index) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "    print('epoch ', epoch)\n",
        "    display(j) # display the bar\n",
        "    print()\n",
        "    tk1 = enumerate(ques_inputs)\n",
        "    for i, sentence in tk1:\n",
        "        #print(i)\n",
        "        #print(sentence)\n",
        "        loss = 0.\n",
        "\n",
        "        #initialise encoder state vector and cell state vector\n",
        "        h = encoder.init_hidden()\n",
        "        encoder_state_vector = h[0]\n",
        "        encoder_state_vector = encoder_state_vector.to('cuda')\n",
        "        encoder_cell_vector = h[0]\n",
        "        encoder_cell_vector = encoder_cell_vector.to('cuda')\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        inp = torch.tensor(sentence).unsqueeze(0).to('cuda')\n",
        "\n",
        "        #print('inp: ', epoch, inp)\n",
        "        #if (i % 1000) == 0:\n",
        "        #  print('inp: ', i, epoch)\n",
        "        encoder_outputs, h = encoder(inp, encoder_state_vector, encoder_cell_vector)\n",
        "\n",
        "        #First decoder input will be the SOS token\n",
        "        decoder_input = torch.tensor([ques_w2i['_SOS']]).to('cuda')\n",
        "        #First decoder hidden state will be last encoder hidden state\n",
        "        decoder_hidden = h\n",
        "\n",
        "        output = []\n",
        "        teacher_forcing = True if random.random() < teacher_forcing_prob else False\n",
        "        #k = IntProgress(min=0, max=len(ans_inputs[i])) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "        #k.value=0\n",
        "        #display(k) # display the bar\n",
        "\n",
        "        for ii in range(len(ans_inputs[i])):\n",
        "          decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden[0], decoder_hidden[1])\n",
        "\n",
        "          # Get the index value of the word with the highest score from the decoder output\n",
        "          top_value, top_index = decoder_output.topk(1)\n",
        "          if teacher_forcing:\n",
        "            decoder_input = torch.tensor([ans_inputs[i][ii]]).to('cuda')\n",
        "          else:\n",
        "            decoder_input = torch.tensor([top_index.item()]).to('cuda')\n",
        "\n",
        "          output.append(top_index.item())\n",
        "          # Calculate the loss of the prediction against the actual word\n",
        "          loss += F.nll_loss(decoder_output.view(1,-1), torch.tensor([ans_inputs[i][ii]]).to('cuda'))\n",
        "          j.value += 1\n",
        "          k.value += 1\n",
        "          #print(j.value)\n",
        "          #print((j.value/(len(ans_inputs) * max_index))*100)\n",
        "\n",
        "        loss.backward()\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "        avg_loss += loss.item()/len(ques_inputs)\n",
        "    print(avg_loss)\n",
        "    torch.save({\"encoder\":encoder.state_dict(),\"decoder\":decoder.state_dict(),\"q_optimizer\":encoder_optimizer.state_dict(),\"a_optimizer\":decoder_optimizer},\"model_enc_dec.pt\")\n",
        "\n",
        "# Save model after every epoch (Optional)\n",
        "#torch.save({\"encoder\":encoder.state_dict(),\"decoder\":decoder.state_dict(),\"q_optimizer\":encoder_optimizer.state_dict(),\"a_optimizer\":decoder_optimizer},\"model_enc_dec.pt\")\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to train model: ',end-start)"
      ],
      "metadata": {
        "id": "W_w-5Xuxg8uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "checkpoint = torch.load(\"model_enc_dec.pt\")\n",
        "\n",
        "encoder.load_state_dict(checkpoint['encoder'])\n",
        "decoder.load_state_dict(checkpoint['decoder'])\n",
        "encoder_optimizer.load_state_dict(checkpoint['q_optimizer'])\n",
        "#decoder_optimizer.load_state_dict(checkpoint['a_optimizer'])\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# get some random numbers to choose random sentences\n",
        "rand_integers = [random.randint(0, 1000) for i in range(1, 20)]\n",
        "\n",
        "for i in rand_integers:\n",
        "  h = encoder.init_hidden()\n",
        "  inp = torch.tensor(ques_inputs[i]).unsqueeze(0).to('cuda')\n",
        "  encoder_outputs, h = encoder(inp, h[0].to('cuda'), h[1].to('cuda'))\n",
        "  print('inp:',inp)\n",
        "  print('en_inputs[',i,']: ', ques_inputs[i])\n",
        "  decoder_input = torch.tensor([ques_w2i['_SOS']]).to('cuda')\n",
        "  decoder_hidden = h\n",
        "  output = []\n",
        "  attentions = []\n",
        "  while True:\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden[0], decoder_hidden[1])\n",
        "    _, top_index = decoder_output.topk(1)\n",
        "    decoder_input = torch.tensor([top_index.item()]).to('cuda')\n",
        "    # If the decoder output is the End Of Sentence token, stop decoding process\n",
        "    if top_index.item() == ans_w2i[\"_EOS\"]:\n",
        "      break\n",
        "    output.append(top_index.item())\n",
        "\n",
        "  print(\"Question: \"+ \" \".join([ques_i2w[x] for x in ques_inputs[i]]))\n",
        "  print(\"Answer Predicted: \" + \" \".join([ans_i2w[x] for x in output]))\n",
        "  print(\"Actual: \" + \" \".join([ans_i2w[x] for x in ans_inputs[i]]))\n",
        "  print()\n",
        "\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to test model: ',end-start)\n",
        "\n"
      ],
      "metadata": {
        "id": "h7asNohDkPkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pvZMfMb3o0CW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}