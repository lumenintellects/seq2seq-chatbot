{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaE7wSj0p5J5sqCHvbzVT+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lumenintellects/seq2seq-chatbot/blob/main/CSK507_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Qgwlz2unHMw8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import datetime\n",
        "\n",
        "PATH_WORKSPACE_ROOT = r'.' # Set the workspace root path here\n",
        "FOLDER_DATASET = 'dataset' # Set the name of the folder containing the datasets here\n",
        "FOLDER_LOG = FOLDER_DATASET # Set the name of the folder containing the log files here\n",
        "\n",
        "EXTENSION_CSV = '.csv'\n",
        "EXTENSION_PT = '.pt'\n",
        "EXTENSION_PKL = '.pkl'\n",
        "EXTENSION_JSON = '.json'\n",
        "EXTENSION_LOG = '.log'\n",
        "EXTENSION_PTH = '.pth'\n",
        "\n",
        "FILE_NAME_DELIMITER = '_'\n",
        "\n",
        "def to_csv_filename(name):\n",
        "    \"\"\"\n",
        "    Create a CSV filename with the given name.\n",
        "\n",
        "    Parameters:\n",
        "        name (str): The name of the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        str: The full path to the CSV file.\n",
        "    \"\"\"\n",
        "    return f\"{name}{EXTENSION_CSV}\"\n",
        "\n",
        "def to_pt_filename(name):\n",
        "    \"\"\"\n",
        "    Create a PT filename with the given name.\n",
        "\n",
        "    Parameters:\n",
        "        name (str): The name of the PT file.\n",
        "\n",
        "    Returns:\n",
        "        str: The full path to the PT file.\n",
        "    \"\"\"\n",
        "    return f\"{name}{EXTENSION_PT}\"\n",
        "\n",
        "def to_pkl_filename(name):\n",
        "    \"\"\"\n",
        "    Create a PKL filename with the given name.\n",
        "\n",
        "    Parameters:\n",
        "        name (str): The name of the PKL file.\n",
        "\n",
        "    Returns:\n",
        "        str: The full path to the PKL file.\n",
        "    \"\"\"\n",
        "    return f\"{name}{EXTENSION_PKL}\"\n",
        "\n",
        "def to_json_filename(name):\n",
        "    \"\"\"\n",
        "    Create a JSON filename with the given name.\n",
        "\n",
        "    Parameters:\n",
        "        name (str): The name of the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        str: The full path to the JSON file.\n",
        "    \"\"\"\n",
        "    return f\"{name}{EXTENSION_JSON}\"\n",
        "\n",
        "def to_log_filename(name):\n",
        "    \"\"\"\n",
        "    Create a log filename with the given name.\n",
        "\n",
        "    Parameters:\n",
        "        name (str): The name of the log file.\n",
        "\n",
        "    Returns:\n",
        "        str: The full path to the log file.\n",
        "    \"\"\"\n",
        "    return f\"{name}{EXTENSION_LOG}\"\n",
        "\n",
        "def to_pth_filename(name):\n",
        "    \"\"\"\n",
        "    Create a PTH filename with the given name.\n",
        "\n",
        "    Parameters:\n",
        "        name (str): The name of the PTH file.\n",
        "\n",
        "    Returns:\n",
        "        str: The full path to the PTH file.\n",
        "    \"\"\"\n",
        "    return f\"{name}{EXTENSION_PTH}\"\n",
        "\n",
        "def compose_filename(base, name_tokens):\n",
        "    \"\"\"\n",
        "    Compose a filename with the given base and name tokens.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "        name_tokens (list): A list of name tokens.\n",
        "\n",
        "    Returns:\n",
        "        str: The composed filename.\n",
        "    \"\"\"\n",
        "    return FILE_NAME_DELIMITER.join([base] + name_tokens)\n",
        "\n",
        "NAME_TOKEN_OUTLIERS = 'outliers'\n",
        "NAME_TOKEN_INPUT_OUTPUT_PAIRS = 'input_output_pairs'\n",
        "NAME_TOKEN_INPUT = 'input'\n",
        "NAME_TOKEN_OUTPUT = 'output'\n",
        "NAME_TOKEN_VOCAB = 'vocab'\n",
        "NAME_TOKEN_SEQ = 'seq'\n",
        "NAME_TOKEN_PADDED = 'padded'\n",
        "NAME_TOKEN_BATCH = 'batch'\n",
        "\n",
        "def get_path_log(base, dataset_name, timestamp_token):\n",
        "    \"\"\"\n",
        "    Get the path to the log file.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "        timestamp_token (str): The timestamp token.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the log file.\n",
        "    \"\"\"\n",
        "    base_filename = compose_filename(base, [dataset_name, timestamp_token])\n",
        "    log_filename = to_log_filename(base_filename)\n",
        "    return os.path.join(FOLDER_LOG, log_filename)\n",
        "\n",
        "def get_path_source_csv(base):\n",
        "    \"\"\"\n",
        "    Get the path to the source CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the source CSV file.\n",
        "    \"\"\"\n",
        "\n",
        "    base_filename = compose_filename(base, [])\n",
        "    csv_filename = to_csv_filename(base_filename)\n",
        "    return os.path.join(FOLDER_DATASET, csv_filename)\n",
        "\n",
        "def get_path_outliers(base):\n",
        "    \"\"\"\n",
        "    Get the path to the outliers CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the outliers CSV file.\n",
        "    \"\"\"\n",
        "    base_filename = compose_filename(base, [NAME_TOKEN_OUTLIERS])\n",
        "    csv_filename = to_csv_filename(base_filename)\n",
        "    return os.path.join(FOLDER_DATASET, csv_filename)\n",
        "\n",
        "def get_path_input_output_pairs(base):\n",
        "    \"\"\"\n",
        "    Get the path to the input-output pairs file.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the input-output pairs file.\n",
        "    \"\"\"\n",
        "    base_filename = compose_filename(base, [NAME_TOKEN_INPUT_OUTPUT_PAIRS])\n",
        "    csv_filename = to_csv_filename(base_filename)\n",
        "    return os.path.join(FOLDER_DATASET, csv_filename)\n",
        "\n",
        "def get_path_input_sequences(base):\n",
        "    \"\"\"\n",
        "    Get the path to the input sequences file.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the input sequences file.\n",
        "    \"\"\"\n",
        "    base_filename = compose_filename(base, [NAME_TOKEN_INPUT, NAME_TOKEN_SEQ])\n",
        "    pt_filename = to_pt_filename(base_filename)\n",
        "    return os.path.join(FOLDER_DATASET, pt_filename)\n",
        "\n",
        "def get_path_output_sequences(base):\n",
        "    \"\"\"\n",
        "    Get the path to the output sequences file.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the output sequences file.\n",
        "    \"\"\"\n",
        "    base_filename = compose_filename(base, [NAME_TOKEN_OUTPUT, NAME_TOKEN_SEQ])\n",
        "    pt_filename = to_pt_filename(base_filename)\n",
        "    return os.path.join(FOLDER_DATASET, pt_filename)\n",
        "\n",
        "def get_path_vocab(base):\n",
        "    \"\"\"\n",
        "    Get the path to the vocabulary file.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the vocabulary file.\n",
        "    \"\"\"\n",
        "    base_filename = compose_filename(base, [NAME_TOKEN_VOCAB])\n",
        "    pkl_filename = to_pkl_filename(base_filename)\n",
        "    return os.path.join(FOLDER_DATASET, pkl_filename)\n",
        "\n",
        "def get_path_input_sequences_padded_batch(base, batch_number):\n",
        "    \"\"\"\n",
        "    Get the path to the input sequences padded batch file.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "        batch_number (int): The batch number.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the input sequences padded batch file.\n",
        "    \"\"\"\n",
        "    base_filename = compose_filename(base, [NAME_TOKEN_INPUT, NAME_TOKEN_SEQ, NAME_TOKEN_PADDED, NAME_TOKEN_BATCH, str(batch_number)])\n",
        "    pt_filename = to_pt_filename(base_filename)\n",
        "    return os.path.join(FOLDER_DATASET, pt_filename)\n",
        "\n",
        "def get_path_output_sequences_padded_batch(base, batch_number):\n",
        "    \"\"\"\n",
        "    Get the path to the output sequences padded batch file.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "        batch_number (int): The batch number.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the output sequences padded batch file.\n",
        "    \"\"\"\n",
        "    base_filename = compose_filename(base, [NAME_TOKEN_OUTPUT, NAME_TOKEN_SEQ, NAME_TOKEN_PADDED, NAME_TOKEN_BATCH, str(batch_number)])\n",
        "    pt_filename = to_pt_filename(base_filename)\n",
        "    return os.path.join(FOLDER_DATASET, pt_filename)\n",
        "\n",
        "def get_path_input_sequences_padded_batch_pattern(base):\n",
        "    \"\"\"\n",
        "    Get the path pattern for the input sequences padded batch files.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "\n",
        "    Returns:\n",
        "        str: The path pattern for the input sequences padded batch files.\n",
        "    \"\"\"\n",
        "    path_pattern = compose_filename(base, [NAME_TOKEN_INPUT, NAME_TOKEN_SEQ, NAME_TOKEN_PADDED, NAME_TOKEN_BATCH])\n",
        "    return os.path.join(FOLDER_DATASET, f\"{path_pattern}{FILE_NAME_DELIMITER}*.pt\")\n",
        "\n",
        "def get_path_output_sequences_padded_batch_pattern(base):\n",
        "    \"\"\"\n",
        "    Get the path pattern for the output sequences padded batch files.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "\n",
        "    Returns:\n",
        "        str: The path pattern for the output sequences padded batch files.\n",
        "    \"\"\"\n",
        "    path_pattern = compose_filename(base, [NAME_TOKEN_OUTPUT, NAME_TOKEN_SEQ, NAME_TOKEN_PADDED, NAME_TOKEN_BATCH])\n",
        "    return os.path.join(FOLDER_DATASET, f\"{path_pattern}{FILE_NAME_DELIMITER}*.pt\")\n",
        "\n",
        "def get_path_model(base, version):\n",
        "    \"\"\"\n",
        "    Get the path to the model file.\n",
        "\n",
        "    Parameters:\n",
        "        base (str): The base filename.\n",
        "        version (str): The version of the model.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the model file.\n",
        "    \"\"\"\n",
        "    base_filename = compose_filename(base, [version])\n",
        "    pth_filename = to_pth_filename(base_filename)\n",
        "    return os.path.join(FOLDER_DATASET, pth_filename)\n",
        "\n",
        "MODE_READONLY = 'r'\n",
        "\n",
        "BASE_FILENAME_SETTINGS = 'settings' # Set the base filename for the settings JSON file here\n",
        "BASE_FILENAME_MODEL = 'seq2seq_model'\n",
        "\n",
        "# Define the settings keys inside the settings JSON file\n",
        "SETTING_ENABLE_LOGGING = 'enableLogging'\n",
        "SETTING_TRAINING_LOOP_CONTINUE = 'trainingLoopContinue'\n",
        "SETTING_NEXT_SUBSET_CONTINUE = 'nextSubsetContinue'\n",
        "SETTING_ANALYZE_SEQUENCES = 'analyzeSequences'\n",
        "SETTING_DEBUG_MODE = 'debugMode'\n",
        "SETTING_TRAINING_SUBSET_SIZE = 'trainingSubsetSize'\n",
        "SETTING_EVALUATION_SUBSET_SIZE = 'evaluationSubsetSize'\n",
        "SETTING_EVALUATION_LOOP_CONTINUE = 'evaluationLoopContinue'\n",
        "SETTING_EVALUATION_RELOAD_MODEL_IN_LOOP = 'evaluationReloadModelInLoop'\n",
        "\n",
        "# Define the Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src shape: (batch_size, src_len)\n",
        "        embedded = self.dropout(self.embedding(src))  # (batch_size, src_len, emb_dim)\n",
        "        outputs, hidden = self.rnn(embedded)  # outputs: (batch_size, src_len, hidden_dim), hidden: (n_layers, batch_size, hidden_dim)\n",
        "        return hidden  # Return only the hidden state\n",
        "\n",
        "# Define the Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, hidden):\n",
        "        # trg shape: (batch_size, trg_len)\n",
        "        # hidden shape: (n_layers, batch_size, hidden_dim)\n",
        "        embedded = self.dropout(self.embedding(trg))  # (batch_size, trg_len, emb_dim)\n",
        "        outputs, hidden = self.rnn(embedded, hidden)  # (batch_size, trg_len, hidden_dim), hidden: (n_layers, batch_size, hidden_dim)\n",
        "        predictions = self.fc_out(outputs).float()  # Ensure predictions are float32\n",
        "        return predictions, hidden\n",
        "\n",
        "# Define the Seq2Seq model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # src: (batch_size, src_len), trg: (batch_size, trg_len)\n",
        "        hidden = self.encoder(src)  # Get the context vector\n",
        "        outputs, _ = self.decoder(trg, hidden)  # Decode based on the context vector\n",
        "        return outputs\n",
        "\n",
        "class Seq2SeqWithAttention(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src: [src_len, batch_size]\n",
        "        # trg: [trg_len, batch_size]\n",
        "        # teacher_forcing_ratio: probability to use teacher forcing\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Encode the source sequence\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "\n",
        "        # First input to the decoder is the <bos> token\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            # Decode\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Decide whether to use teacher forcing\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "\n",
        "            # Get the highest predicted token\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            # Decide the next input\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "def get_setting(setting_name):\n",
        "    \"\"\"\n",
        "    Get the value of a setting from the settings file.\n",
        "\n",
        "    Parameters:\n",
        "        setting_name (str): The name of the setting.\n",
        "\n",
        "    Returns:\n",
        "        str: The value of the setting.\n",
        "    \"\"\"\n",
        "    filename_settings = to_json_filename(BASE_FILENAME_SETTINGS)\n",
        "    relative_path_settings = os.path.join(filename_settings)\n",
        "    path_settings = os.path.join(PATH_WORKSPACE_ROOT, relative_path_settings)\n",
        "\n",
        "    # Load the settings JSON file\n",
        "    with open(path_settings, MODE_READONLY) as file:\n",
        "        settings = json.load(file)\n",
        "        return settings[setting_name]\n",
        "\n",
        "def get_setting_training_subset_size():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'trainingSubsetSize' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        int: The value of the setting 'trainingSubsetSize'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_TRAINING_SUBSET_SIZE)\n",
        "\n",
        "\n",
        "def get_setting_evaluation_subset_size():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'evaluationSubsetSize' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        int: The value of the setting 'evaluationSubsetSize'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_EVALUATION_SUBSET_SIZE)\n",
        "\n",
        "def get_setting_evaluation_loop_continue():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'evaluationLoopContinue' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        bool: The value of the setting 'evaluationLoopContinue'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_EVALUATION_LOOP_CONTINUE)\n",
        "\n",
        "def get_setting_evaluation_reload_model_in_loop():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'evaluationReloadModelInLoop' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        bool: The value of the setting 'evaluationReloadModelInLoop'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_EVALUATION_RELOAD_MODEL_IN_LOOP)\n",
        "\n",
        "def get_setting_evaluation_subset_size():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'evaluationSubsetSize' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        int: The value of the setting 'evaluationSubsetSize'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_EVALUATION_SUBSET_SIZE)\n",
        "\n",
        "def get_setting_evaluation_loop_continue():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'evaluationLoopContinue' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        bool: The value of the setting 'evaluationLoopContinue'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_EVALUATION_LOOP_CONTINUE)\n",
        "\n",
        "def get_setting_evaluation_reload_model_in_loop():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'evaluationReloadModelInLoop' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        bool: The value of the setting 'evaluationReloadModelInLoop'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_EVALUATION_RELOAD_MODEL_IN_LOOP)\n",
        "\n",
        "def get_setting_debug_mode():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'debugMode' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        bool: The value of the setting 'debugMode'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_DEBUG_MODE)\n",
        "\n",
        "def get_setting_analyze_sequences():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'analyzeSequences' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        bool: The value of the setting 'analyzeSequences'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_ANALYZE_SEQUENCES)\n",
        "\n",
        "def get_setting_enable_logging():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'enableLogging' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        bool: The value of the setting 'enableLogging'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_ENABLE_LOGGING)\n",
        "\n",
        "def get_setting_training_loop_continue():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'trainingLoopContinue' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        bool: The value of the setting 'trainingLoopContinue'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_TRAINING_LOOP_CONTINUE)\n",
        "\n",
        "def get_setting_next_subset_continue():\n",
        "    \"\"\"\n",
        "    Get the value of the setting 'nextSubsetContinue' from the settings file.\n",
        "\n",
        "    Returns:\n",
        "        bool: The value of the setting 'nextSubsetContinue'.\n",
        "    \"\"\"\n",
        "    return get_setting(SETTING_NEXT_SUBSET_CONTINUE)\n",
        "\n",
        "def extract_rows(input_csv, output_csv, num_rows):\n",
        "    \"\"\"\n",
        "    Extract the first X rows from a CSV file and save to a new file.\n",
        "\n",
        "    Parameters:\n",
        "        input_csv (str): Path to the input CSV file.\n",
        "        output_csv (str): Path to save the output CSV file.\n",
        "        num_rows (int): Number of rows to extract.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(input_csv)\n",
        "\n",
        "        # Extract the first num_rows rows\n",
        "        sampled_df = df.head(num_rows)\n",
        "\n",
        "        # Save the extracted rows to a new CSV file\n",
        "        sampled_df.to_csv(output_csv, index=False)\n",
        "\n",
        "        print(f\"Successfully extracted {num_rows} rows to {output_csv}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# for each row in the input csv, evaluate each cell against the filter_predicate and write to output csv\n",
        "def filter_csv(input_csv, output_csv, filter_predicate):\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(input_csv)\n",
        "\n",
        "        # Filter the dataset\n",
        "        filtered_df = df[df.apply(filter_predicate, axis=1)]\n",
        "\n",
        "        # Save the filtered dataset to a new CSV file\n",
        "        filtered_df.to_csv(output_csv, index=False)\n",
        "\n",
        "        print(f\"Successfully filtered dataset to {output_csv}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean the text by:\n",
        "    1. removing leading/trailing spaces\n",
        "    2. converting to lowercase.\n",
        "    3. remove excess whitespace chars\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    return \" \".join(text.strip().lower().split())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df_time' not in locals():\n",
        "       df_time = pd.DataFrame(columns=['method','Time_Taken'])"
      ],
      "metadata": {
        "id": "Cls29dWwroSZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import time\n",
        "import logging\n",
        "import pandas as pd\n",
        "import spacy\n",
        "#from common import PATH_WORKSPACE_ROOT, get_path_log, get_path_input_output_pairs, get_path_vocab\n",
        "#from common import get_path_input_sequences, get_path_output_sequences\n",
        "#from common import get_path_input_sequences_padded_batch, get_path_output_sequences_padded_batch\n",
        "#from common import get_path_input_sequences_padded_batch_pattern, get_path_output_sequences_padded_batch_pattern\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "DATASET_NAME = 'ubuntu_dialogue_corpus_input_output_pairs'\n",
        "LOG_BASE_FILENAME = \"3_tokenize_dataset\"\n",
        "\n",
        "N_PROCESS_VALUE = 10\n",
        "BATCH_SIZE = 500000\n",
        "TRAINING_SUBSET_SIZE = 200\n",
        "SETTING_ANALYZE_SEQUENCES = False\n",
        "LOSS_THRESHOLD = 1.0\n",
        "\n",
        "# Tokenizer using spaCy with multithreading\n",
        "def spacy_tokenizer_pipe(texts, nlp, n_process=4):\n",
        "    \"\"\"\n",
        "    Tokenizes a list of texts using SpaCy's nlp.pipe for multithreaded tokenization.\n",
        "\n",
        "    Parameters:\n",
        "        texts (list): List of text strings to tokenize.\n",
        "        nlp: SpaCy language model.\n",
        "        n_process (int): Number of processes for parallel processing.\n",
        "\n",
        "    Returns:\n",
        "        list: List of tokenized texts.\n",
        "    \"\"\"\n",
        "    print(f\"Tokenizing {len(texts)} texts using {n_process} processes...\")\n",
        "    tokenized_texts = []\n",
        "    for doc in nlp.pipe(texts, n_process=n_process):\n",
        "        tokenized_texts.append([token.text for token in doc if not token.is_space])\n",
        "    return tokenized_texts\n",
        "\n",
        "# Build vocabulary from spaCy tokens\n",
        "def build_vocab(tokens_iterable, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]):\n",
        "    vocab = {\"<unk>\": 0, \"<pad>\": 1, \"<bos>\": 2, \"<eos>\": 3}\n",
        "    for tokens in tokens_iterable:\n",
        "        for token in tokens:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "# Process text into sequences of indices with SpaCy pipeline\n",
        "def process_text_spacy_pipe(texts, vocab, nlp, n_process=4):\n",
        "    \"\"\"\n",
        "    Tokenizes a list of texts using SpaCy's nlp.pipe for multithreaded processing\n",
        "    and converts them to sequences of indices.\n",
        "\n",
        "    Parameters:\n",
        "        texts (list): List of text strings to process.\n",
        "        vocab (dict): Vocabulary mapping tokens to indices.\n",
        "        nlp: SpaCy language model.\n",
        "        n_process (int): Number of processes for parallel processing.\n",
        "\n",
        "    Returns:\n",
        "        list: List of sequences of indices.\n",
        "    \"\"\"\n",
        "    print(f\"Processing {len(texts)} texts using {n_process} processes...\")\n",
        "    tokenized_sequences = []\n",
        "    for doc in nlp.pipe(texts, n_process=n_process):\n",
        "        tokens = [\"<bos>\"] + [token.text for token in doc if not token.is_space] + [\"<eos>\"]\n",
        "        tokenized_sequences.append([vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\n",
        "    return tokenized_sequences\n",
        "\n",
        "def analyze_sequences(sequences):\n",
        "    sequence_lengths = [len(seq) for seq in sequences]\n",
        "    max_length = max(sequence_lengths)\n",
        "    mean_length = sum(sequence_lengths) / len(sequence_lengths)\n",
        "    median_length = sorted(sequence_lengths)[len(sequence_lengths) // 2]\n",
        "\n",
        "    # Percentiles\n",
        "    import numpy as np\n",
        "    percentile_95 = np.percentile(sequence_lengths, 95)\n",
        "\n",
        "    print(f\"Max Length: {max_length}\")\n",
        "    print(f\"Mean Length: {mean_length}\")\n",
        "    print(f\"Median Length: {median_length}\")\n",
        "    print(f\"95th Percentile: {percentile_95}\")\n",
        "\n",
        "# Explicitly pad sequences to the global maximum length\n",
        "def pad_to_length(sequences, max_length, padding_value):\n",
        "    \"\"\"\n",
        "    Pads all sequences to a specified maximum length.\n",
        "\n",
        "    Parameters:\n",
        "        sequences (list of lists): Sequences to pad.\n",
        "        max_length (int): Desired maximum length.\n",
        "        padding_value (int): Padding value.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of padded sequences.\n",
        "    \"\"\"\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) > max_length:\n",
        "            seq = seq[:max_length]  # Truncate if longer than max_length\n",
        "        else:\n",
        "            seq = seq + [padding_value] * (max_length - len(seq))  # Pad if shorter\n",
        "        padded_sequences.append(seq)\n",
        "    return torch.tensor(padded_sequences, dtype=torch.int64)\n",
        "\n"
      ],
      "metadata": {
        "id": "MHiUPvDVJgsc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Set the current working directory\n",
        "    os.chdir(PATH_WORKSPACE_ROOT)\n",
        "\n",
        "    log_start_time = time.strftime('%Y%m%d_%H%M%S')\n",
        "    path_log = get_path_log(LOG_BASE_FILENAME, DATASET_NAME, log_start_time)\n",
        "\n",
        "    # Set up logging configuration\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,  # Set the minimum log level (DEBUG, INFO, WARNING, etc.)\n",
        "        format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Log format with timestamps\n",
        "        handlers=[\n",
        "            logging.FileHandler(path_log),  # Log to a file\n",
        "            logging.StreamHandler()  # Log to the console\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    print(\"Running main script...\")\n",
        "    print(f\"Current Working Directory: {os.getcwd()}\")\n",
        "\n",
        "    # Load spaCy language model\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"spaCy model loaded.\")\n",
        "\n",
        "    # ==========================\n",
        "\n",
        "    path_input_output_pairs = get_path_input_output_pairs(DATASET_NAME)\n",
        "    path_vocab = get_path_vocab(DATASET_NAME)\n",
        "    path_input_sequences = get_path_input_sequences(DATASET_NAME)\n",
        "    path_output_sequences = get_path_output_sequences(DATASET_NAME)\n",
        "    path_input_sequences_padded_batch_pattern = get_path_input_sequences_padded_batch_pattern(DATASET_NAME)\n",
        "    path_output_sequences_padded_batch_pattern = get_path_output_sequences_padded_batch_pattern(DATASET_NAME)\n",
        "\n",
        "    # Define the save path\n",
        "    path_model = os.path.join(PATH_WORKSPACE_ROOT, \"seq2seq_model.pth\")\n",
        "\n",
        "    # ==========================\n",
        "\n",
        "    # Load the dataset#####################################################################################################################################\n",
        "    df = pd.read_csv(path_input_output_pairs,on_bad_lines='skip',engine='python')\n",
        "    print(f\"Loaded csv into dataframe: {path_input_output_pairs}\")\n",
        "    df_main = df\n",
        "    df = df.loc[0:100]\n",
        "    # Replace NaN in 'input' and 'output' columns\n",
        "    df['input'] = df['input'].fillna(\"\")\n",
        "    df['output'] = df['output'].fillna(\"\")\n",
        "    print(\"NaN replaced with empty strings.\")\n",
        "\n",
        "    # Check for existing vocabulary\n",
        "    if os.path.exists(path_vocab):\n",
        "        print(\"Vocabulary file found. Loading vocabulary...\")\n",
        "        with open(path_vocab, \"rb\") as vocab_file:\n",
        "            vocab = pickle.load(vocab_file)\n",
        "        print(f\"Vocabulary loaded. Size: {len(vocab)}\")\n",
        "    else:\n",
        "        print(\"Vocabulary file not found. Generating vocabulary...\")\n",
        "\n",
        "        # Tokenize using SpaCy's multithreading\n",
        "        print(\"Tokenizing input and output texts...\")\n",
        "        time_input_sequences_start = time.time()\n",
        "        time_input_sequences_start_hh_mm_ss = time.strftime('%H:%M:%S', time.localtime(time_input_sequences_start))\n",
        "        print(f\"The time is: {time_input_sequences_start_hh_mm_ss}\")\n",
        "        texts_combined = df['input'].tolist() + df['output'].tolist()\n",
        "        combined_tokens = spacy_tokenizer_pipe(texts_combined, nlp, n_process=N_PROCESS_VALUE)\n",
        "        time_input_sequences_end = time.time()\n",
        "        print(f\"Tokenization completed in {time_input_sequences_end - time_input_sequences_start:.2f} seconds.\")\n",
        "\n",
        "        # Build vocabulary from tokenized texts\n",
        "        print(\"Building vocabulary...\")\n",
        "        time_build_vocab_start = time.time()\n",
        "        time_build_vocab_start_hh_mm_ss = time.strftime('%H:%M:%S', time.localtime(time_build_vocab_start))\n",
        "        print(f\"The time is: {time_build_vocab_start_hh_mm_ss}\")\n",
        "        vocab = build_vocab(combined_tokens)\n",
        "        time_build_vocab_end = time.time()\n",
        "        print(f\"Vocabulary built in {time_build_vocab_end - time_build_vocab_start} seconds.\")\n",
        "        print(f\"Vocabulary built. Size: {len(vocab)}\")\n",
        "\n",
        "        # Save the vocabulary to file\n",
        "        with open(path_vocab, \"wb\") as vocab_file:\n",
        "            pickle.dump(vocab, vocab_file)\n",
        "        print(\"Vocabulary saved to file.\")\n",
        "\n",
        "    padding_value = vocab[\"<pad>\"]\n",
        "\n",
        "    # Check for previous serialized input sequences\n",
        "    if os.path.exists(path_input_sequences):\n",
        "        print(\"Serialized input sequences found.\")\n",
        "    else:\n",
        "        print(\"Serialized input sequences not found, generating input sequences...\")\n",
        "\n",
        "        # Tokenize and convert to sequences\n",
        "        print(\"Tokenizing and converting to input sequences...\")\n",
        "\n",
        "        input_texts = df['input'].tolist()\n",
        "\n",
        "        # Process input sequences in parallel\n",
        "        time_input_sequences_start = time.time()\n",
        "        time_input_sequences_start_hh_mm_ss = time.strftime('%H:%M:%S', time.localtime(time_input_sequences_start))\n",
        "        print(f\"The time is: {time_input_sequences_start_hh_mm_ss}\")\n",
        "        input_sequences = process_text_spacy_pipe(input_texts, vocab, nlp, n_process=N_PROCESS_VALUE)\n",
        "        torch.save(input_sequences, path_input_sequences)\n",
        "        time_input_sequences_end = time.time()\n",
        "        print(f\"Input sequences completed in {time_input_sequences_end - time_input_sequences_start} seconds.\")\n",
        "\n",
        "    # Check for previous serialized padded input sequences matching batch file name pattern\n",
        "    if len(glob.glob(path_input_sequences_padded_batch_pattern)) > 0:\n",
        "        print(\"Serialized padded input sequences found.\")\n",
        "    else:\n",
        "        input_sequences = torch.load(path_input_sequences, weights_only=True)\n",
        "\n",
        "        input_lengths = [len(seq) for seq in input_sequences]\n",
        "        input_max_length = max(input_lengths)\n",
        "        print(f\"Max input length: {input_max_length}\")\n",
        "\n",
        "        input_mean_length = sum(input_lengths) / len(input_lengths)\n",
        "        print(f\"Mean input length: {input_mean_length}\")\n",
        "\n",
        "        input_median_length = sorted(input_lengths)[len(input_lengths) // 2]\n",
        "        print(f\"Median input length: {input_median_length}\")\n",
        "\n",
        "        input_percentile_95 = np.percentile(input_lengths, 95)\n",
        "        print(f\"95th percentile input length: {input_percentile_95}\")\n",
        "\n",
        "        # truncate input sequences longer than the 95th percentile\n",
        "        print(\"Truncating input sequences longer than the 95th percentile...\")\n",
        "        input_max_length = int(input_percentile_95)\n",
        "        input_sequences = [seq[:input_max_length] for seq in input_sequences]\n",
        "\n",
        "        print(\"Serialized padded input sequences not found, padding input sequences...\")\n",
        "        time_pad_input_sequences_start = time.time()\n",
        "        time_pad_input_sequences_start_hh_mm_ss = time.strftime('%H:%M:%S', time.localtime(time_pad_input_sequences_start))\n",
        "        print(f\"The time is: {time_pad_input_sequences_start_hh_mm_ss}\")\n",
        "\n",
        "        # Process sequences in batches to avoid memory issues\n",
        "        for i in range(0, len(input_sequences), BATCH_SIZE):\n",
        "            batch = input_sequences[i:i + BATCH_SIZE]\n",
        "            print(f\"Padding sequences in batch {i // BATCH_SIZE} to {input_max_length}\")\n",
        "            padded_batch = pad_to_length(batch, input_max_length, padding_value)  # Use explicit padding\n",
        "\n",
        "            # Examine the padded batch\n",
        "            print(f\"Batch {i // BATCH_SIZE} shape: {padded_batch.shape}\")\n",
        "\n",
        "            batch_file_path = get_path_input_sequences_padded_batch(DATASET_NAME, i // BATCH_SIZE)\n",
        "            torch.save(padded_batch, batch_file_path)\n",
        "            print(f\"Saved batch {i // BATCH_SIZE} to {batch_file_path}\")\n",
        "\n",
        "        time_pad_input_sequences_end = time.time()\n",
        "        print(f\"Padding input sequences completed in {time_pad_input_sequences_end - time_pad_input_sequences_start} seconds.\")\n",
        "\n",
        "    # Check for previous serialized output sequences\n",
        "    if os.path.exists(path_output_sequences):\n",
        "        print(\"Serialized output sequences found.\")\n",
        "    else:\n",
        "        print(\"Serialized output sequences not found, generating output sequences...\")\n",
        "\n",
        "        # Tokenize and convert to sequences\n",
        "        print(\"Tokenizing and converting to output sequences...\")\n",
        "\n",
        "        output_texts = df['output'].tolist()\n",
        "\n",
        "        # Process output sequences in parallel\n",
        "        time_output_sequences_start = time.time()\n",
        "        time_output_sequences_start_hh_mm_ss = time.strftime('%H:%M:%S', time.localtime(time_output_sequences_start))\n",
        "        print(f\"The time is: {time_output_sequences_start_hh_mm_ss}\")\n",
        "        output_sequences = process_text_spacy_pipe(output_texts, vocab, nlp, n_process=N_PROCESS_VALUE)\n",
        "        torch.save(output_sequences, path_output_sequences)\n",
        "        time_output_sequences_end = time.time()\n",
        "        print(f\"Output sequences completed in {time_output_sequences_end - time_output_sequences_start} seconds.\")\n",
        "\n",
        "    # Check for previous serialized padded output sequences matching batch file name pattern\n",
        "    if len(glob.glob(path_output_sequences_padded_batch_pattern)) > 0:\n",
        "        print(\"Serialized padded output sequences found.\")\n",
        "    else:\n",
        "        output_sequences = torch.load(path_output_sequences, weights_only=True)\n",
        "\n",
        "        output_lengths = [len(seq) for seq in output_sequences]\n",
        "        output_max_length = max(output_lengths)\n",
        "        print(f\"Max output length: output_max_length\")\n",
        "\n",
        "        output_mean_length = sum(output_lengths) / len(output_lengths)\n",
        "        print(f\"Mean output length: {output_mean_length}\")\n",
        "\n",
        "        output_median_length = sorted(output_lengths)[len(output_lengths) // 2]\n",
        "        print(f\"Median output length: {output_median_length}\")\n",
        "\n",
        "        output_percentile_95 = np.percentile(output_lengths, 95)\n",
        "        print(f\"95th percentile output length: {output_percentile_95}\")\n",
        "\n",
        "        # truncate output sequences longer than the 95th percentile\n",
        "        print(\"Truncating output sequences longer than the 95th percentile...\")\n",
        "        output_max_length = int(output_percentile_95)\n",
        "        output_sequences = [seq[:output_max_length] for seq in output_sequences]\n",
        "\n",
        "        print(\"Serialized padded output sequences not found, padding output sequences...\")\n",
        "        time_pad_output_sequences_start = time.time()\n",
        "        time_pad_output_sequences_start_hh_mm_ss = time.strftime('%H:%M:%S', time.localtime(time_pad_output_sequences_start))\n",
        "        print(f\"The time is: {time_pad_output_sequences_start_hh_mm_ss}\")\n",
        "\n",
        "        # Process sequences in batches to avoid memory issues\n",
        "        for i in range(0, len(output_sequences), BATCH_SIZE):\n",
        "            batch = output_sequences[i:i + BATCH_SIZE]\n",
        "            print(f\"Padding sequences in batch {i // BATCH_SIZE} to {output_max_length}\")\n",
        "            padded_batch = pad_to_length(batch, output_max_length, padding_value)  # Use explicit padding\n",
        "\n",
        "            # Examine the padded batch\n",
        "            print(f\"Batch {i // BATCH_SIZE} shape: {padded_batch.shape}\")\n",
        "\n",
        "            batch_file_path = get_path_output_sequences_padded_batch(DATASET_NAME, i // BATCH_SIZE)\n",
        "            torch.save(padded_batch, batch_file_path)\n",
        "            print(f\"Saved batch {i // BATCH_SIZE} to {batch_file_path}\")\n",
        "\n",
        "        time_pad_output_sequences_end = time.time()\n",
        "        print(f\"Padding output sequences completed in {time_pad_output_sequences_end - time_pad_output_sequences_start} seconds.\")\n",
        "        print(\"Exiting program.\")\n",
        "        exit()\n",
        "\n",
        "    input_sequences_padded = torch.cat([torch.load(file, weights_only=True) for file in glob.glob(path_input_sequences_padded_batch_pattern)], dim=0)\n",
        "    print(\"Loaded input sequences from files.\")\n",
        "\n",
        "    output_sequences_padded = torch.cat([torch.load(file, weights_only=True) for file in glob.glob(path_output_sequences_padded_batch_pattern)], dim=0)\n",
        "    print(\"Loaded output sequences from file.\")\n",
        "\n",
        "    # Analyze sequences\n",
        "    if SETTING_ANALYZE_SEQUENCES:\n",
        "        print(\"Analyzing input and output sequences...\")\n",
        "        analyze_sequences(input_sequences_padded)\n",
        "        analyze_sequences(output_sequences_padded)\n",
        "\n",
        "        print(f\"Input shape: {input_sequences_padded.shape}\")\n",
        "        print(f\"Output shape: {output_sequences_padded.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "print(end)\n",
        "\n",
        "print('Time taken to tokenize: ',end-start)\n",
        "time_taken = end - start\n",
        "new_row = pd.DataFrame({'Time_Taken': [time_taken], 'Method': ['multi']})\n",
        "df_time = pd.concat([df_time, new_row], ignore_index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBx_K8iDJmX7",
        "outputId": "29394156-f7eb-4d99-e6aa-e3ec7a9bd29c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 19:50:06.194863\n",
            "Running main script...\n",
            "Current Working Directory: /content\n",
            "spaCy model loaded.\n",
            "Loaded csv into dataframe: dataset/ubuntu_dialogue_corpus_input_output_pairs_input_output_pairs.csv\n",
            "NaN replaced with empty strings.\n",
            "Vocabulary file found. Loading vocabulary...\n",
            "Vocabulary loaded. Size: 739\n",
            "Serialized input sequences found.\n",
            "Serialized padded input sequences found.\n",
            "Serialized output sequences found.\n",
            "Serialized padded output sequences found.\n",
            "Loaded input sequences from files.\n",
            "Loaded output sequences from file.\n",
            "2024-12-08 19:50:18.322518\n",
            "Time taken to tokenize:  0:00:12.127655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-69-249f4b561a67>:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['input'] = df['input'].fillna(\"\")\n",
            "<ipython-input-69-249f4b561a67>:53: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['output'] = df['output'].fillna(\"\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#non multi threaded"
      ],
      "metadata": {
        "id": "3Db3VvmoqyD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "overallStart = datetime.datetime.now()\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "import locale\n",
        "print('Prefferred encoding: ',locale.getpreferredencoding())\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "import os #needed to manipulate the downloaded files within the collab environment and to pull data\n",
        "import json\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import zipfile\n",
        "import spacy.cli\n",
        "from spacy.lang.en import English # updated\n",
        "gpu = spacy.prefer_gpu()\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from ipywidgets import IntProgress\n",
        "from IPython.display import display\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "#because of the complex corpus, use the larger en_core_web_lg\n",
        "gpu = spacy.prefer_gpu()\n",
        "try:\n",
        "  nlp = spacy.load(\"en_core_web_lg\")\n",
        "except OSError:\n",
        "  print('Downloading en_core_web_lg')\n",
        "  spacy.cli.download(\"en_core_web_lg\")\n",
        "  nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "dfu = pd.read_csv('/content/drive/MyDrive/ubuntu_dialogue_corpus_input_output_pairs.csv',on_bad_lines='skip',engine='python')\n",
        "\n",
        "\n",
        "\n",
        "#print('length of df before splitting:', len(dfu))\n",
        "\n",
        "#first_20PCT = int(len(dfu)*0.005)\n",
        "#df = df_main\n",
        "\n",
        "#print('length of df after splitting:', len(df))\n",
        "\n",
        "\n",
        "ques = df['input']\n",
        "ans = df['output']\n",
        "#df = pd.DataFrame({'input': ques, 'output': ans})\n",
        "\n",
        "\n",
        "ques_words = Counter()\n",
        "ans_words = Counter()\n",
        "\n",
        "ques_inputs = []\n",
        "ans_inputs = []\n",
        "\n",
        "f = IntProgress(min=0, max=len(df)) # instantiate the bar  [[McAteer, S (2017) Stackoverflow: How do I implement a progress bar, avaialable at https://stackoverflow.com/a/41457700 (accessed 25/11/2024)]]\n",
        "display(f) # display the bar\n",
        "\n",
        "for i in range(len(df)):\n",
        "    ques_tokens = nlp(str(ques[i]))\n",
        "    ans_tokens = nlp(str(ans[i]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if (len(ques_tokens)!=0 and len(ans_tokens)!=0):\n",
        "\n",
        "\n",
        "      for token in ques_tokens:\n",
        "          ques_words.update([token.text]) #this is the counter for the question frequency, update it\n",
        "\n",
        "\n",
        "      ques_inputs.append([token.text for token in ques_tokens] + ['_EOS'])\n",
        "\n",
        "      for token in ans_tokens:\n",
        "          ans_words.update([token.text]) #this is the counter for the answer frequency, update it\n",
        "\n",
        "      ans_inputs.append([token.text for token in ans_tokens] + ['_EOS'])\n",
        "    f.value += 1\n",
        "\n",
        "\n",
        "end = datetime.datetime.now()\n",
        "\n",
        "print(end)\n",
        "\n",
        "print('Time taken to tokenize: ',end-start)\n",
        "time_taken = end - start\n",
        "new_row = pd.DataFrame({'Time_Taken': [time_taken], 'Method': 'non'})\n",
        "df_time = pd.concat([df_time, new_row], ignore_index=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_ePVE5iKSkv",
        "outputId": "064e19e5-96ff-4859-dedd-37880bb9a6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-08 19:50:18.352244\n",
            "Prefferred encoding:  UTF-8\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "2024-12-08 19:50:22.646675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_time"
      ],
      "metadata": {
        "id": "d7dVLqHTtD-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "\n",
        "# Calculate average time for each method\n",
        "average_times = df_time.groupby('Method')['Time_Taken'].mean().reset_index()\n",
        "title = 'Average Time to Tokenize Comparison over 100 row corpus ('+str(int(len(df_time)/2))+' itterations)'\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x='Time_Taken', y='Method', data=average_times, palette='husl', orient='h')\n",
        "plt.xlabel('Average Time Taken (s)')\n",
        "plt.ylabel('Method')\n",
        "plt.title(title)\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Pve_UDFExO1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CGwXqWaHvope"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}